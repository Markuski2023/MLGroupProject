{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Team description\n",
    "\n",
    "Team: Ehhhhhhh\n",
    "\n",
    "Markus Kinn, 106660\n",
    "Mario Haroun, 543915\n",
    "Torstein Korten, 543955"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5598f9ea683b2936"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def remove_unwanted_rows(df):\n",
    "    unwanted_rows = (df['direct_rad:W'] == 0) & (df['diffuse_rad:W'] == 0) & (df['pv_measurement'] > 200) & (df['sun_elevation:d'] < 0) & (df['is_day:idx'] == 0)\n",
    "    cleaned_df = df[~unwanted_rows]\n",
    "    return cleaned_df\n",
    "def remove_highly_correlated_features(df, threshold):\n",
    "    # Compute the Pearson correlation matrix\n",
    "    correlation_matrix = df.corr(method='pearson')\n",
    "\n",
    "    # Initialize an empty list to hold features to be removed\n",
    "    features_to_remove = []\n",
    "\n",
    "    # Traverse the correlation matrix to find highly correlated features\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            feature1 = correlation_matrix.columns[i]\n",
    "            feature2 = correlation_matrix.columns[j]\n",
    "\n",
    "            # Check for high absolute correlation\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                # Add one of the features to the list if it's not already there\n",
    "                if feature1 not in features_to_remove and feature2 not in features_to_remove:\n",
    "                    features_to_remove.append(feature1)\n",
    "\n",
    "    # Drop the identified features from the DataFrame\n",
    "    filtered_df = df.drop(columns=features_to_remove)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def find_long_constant_periods(data, threshold):\n",
    "    start = None\n",
    "    segments = []\n",
    "    for i in range(1, len(data)):\n",
    "        if data[i] == data[i-1] and data[i] != 0:\n",
    "            if start is None:\n",
    "                start = i-1\n",
    "        else:\n",
    "            if start is not None:\n",
    "                if (i - start) > threshold:\n",
    "                    segments.append((start, i))\n",
    "                start = None\n",
    "    return segments\n",
    "\n",
    "def remove_constant_periods(df, segments):\n",
    "    drop_indices = []\n",
    "    for start, end in segments:\n",
    "        drop_indices.extend(range(start, end))\n",
    "    return df.drop(drop_indices)\n",
    "\n",
    "def lag_features_by_one_hour(df, column_names, time_col='time'):\n",
    "\n",
    "    # Check if the DataFrame has a time-based index\n",
    "    df['index'] = df[time_col]\n",
    "    df = df.set_index('index')\n",
    "\n",
    "    # Loop through each column name to create a lagged feature\n",
    "    for col in column_names:\n",
    "        lagged_col_name = f\"{col}\"\n",
    "        df[lagged_col_name] = df[col].shift(freq='-1H')\n",
    "\n",
    "    return df\n",
    "\n",
    "def is_estimated(df, time_col='time'):\n",
    "    split_date = '2022-10-27'\n",
    "    df['is_estimated'] = 0  # Initialize with 0 (indicating observed)\n",
    "    df.loc[df[time_col] >= pd.Timestamp(split_date), 'is_estimated'] = 1  # Set 1 for estimated data\n",
    "    return df\n",
    "\n",
    "def resample_to_hourly(df, datetime_column='date_forecast'):\n",
    "    df[datetime_column] = pd.to_datetime(df[datetime_column])\n",
    "    df.sort_values(by=datetime_column, inplace=True)\n",
    "\n",
    "    df.set_index(datetime_column, inplace=True)\n",
    "\n",
    "    df_hourly = df.resample('H').mean()\n",
    "\n",
    "    df_hourly.dropna(how='all', inplace=True)\n",
    "\n",
    "    df_hourly.reset_index(inplace=True)\n",
    "\n",
    "    return df_hourly\n",
    "\n",
    "def generate_solar_features_1(data):\n",
    "    relevant_features = [\n",
    "        'direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'sun_elevation:d', 'sun_azimuth:d',\n",
    "        'clear_sky_energy_1h:J', 'direct_rad_1h:J', 'effective_cloud_cover:p', 'diffuse_rad_1h:J',\n",
    "        'is_in_shadow:idx', 'total_cloud_cover:p', 'wind_speed_u_10m:ms', 'snow_water:kgm2',\n",
    "        'relative_humidity_1000hPa:p', 'is_day:idx', 'wind_speed_v_10m:ms', 'cloud_base_agl:m',\n",
    "        'fresh_snow_24h:cm', 'wind_speed_10m:ms', 'pressure_100m:hPa'\n",
    "    ]\n",
    "\n",
    "    interactions = {}\n",
    "    ratios = {}\n",
    "    differences = {}\n",
    "    lags = {}\n",
    "    self_interactions = {}\n",
    "    additive = {}\n",
    "\n",
    "    for col_pair in itertools.combinations(relevant_features, 2):\n",
    "        interactions[f'{col_pair[0]}_times_{col_pair[1]}'] = data[col_pair[0]] * data[col_pair[1]]\n",
    "        ratios[f'{col_pair[0]}_div_{col_pair[1]}'] = data[col_pair[0]] / (data[col_pair[1]] + 1e-8)\n",
    "        differences[f'{col_pair[0]}_minus_{col_pair[1]}'] = data[col_pair[0]] - data[col_pair[1]]\n",
    "        additive[f'{col_pair[0]}_plus_{col_pair[1]}'] = data[col_pair[0]] + data[col_pair[1]]\n",
    "\n",
    "    for col in relevant_features:\n",
    "        self_interactions[f'{col}_squared'] = data[col] ** 2\n",
    "\n",
    "    # Creating lags for all relevant features\n",
    "    for col in relevant_features:\n",
    "        lags[f'{col}_lag1'] = data[col].shift(1)\n",
    "        lags[f'{col}_lag3'] = data[col].shift(3)\n",
    "\n",
    "    # Concatenate all new features with the original data\n",
    "    data = pd.concat([data, pd.DataFrame(interactions), pd.DataFrame(ratios),\n",
    "                      pd.DataFrame(differences), pd.DataFrame(lags), pd.DataFrame(self_interactions), pd.DataFrame(additive)], axis=1)\n",
    "\n",
    "    data['wind_magnitude'] = np.sqrt(data['wind_speed_u_10m:ms']**2 + data['wind_speed_v_10m:ms']**2)\n",
    "    data['wind_direction'] = np.arctan2(data['wind_speed_v_10m:ms'], data['wind_speed_u_10m:ms'])\n",
    "    data['solar_angle_impact'] = np.sin(np.radians(data['sun_elevation:d']))\n",
    "\n",
    "    return data\n",
    "\n",
    "def generate_solar_features_2(data):\n",
    "    relevant_features = [\n",
    "        'sun_elevation:d', 'clear_sky_rad:W', 'direct_rad:W', 'diffuse_rad:W',\n",
    "        'sun_azimuth:d', 'clear_sky_energy_1h:J', 'cloud_base_agl:m', 'diffuse_rad_1h:J',\n",
    "        'effective_cloud_cover:p', 'direct_rad_1h:J', 'snow_water:kgm2', 'is_in_shadow:idx',\n",
    "        'fresh_snow_24h:cm', 'wind_speed_u_10m:ms', 'total_cloud_cover:p', 'msl_pressure:hPa',\n",
    "        'is_day:idx', 'relative_humidity_1000hPa:p', 'pressure_100m:hPa', 'ceiling_height_agl:m'\n",
    "    ]\n",
    "\n",
    "    interactions = {}\n",
    "    ratios = {}\n",
    "    differences = {}\n",
    "    lags = {}\n",
    "    self_interactions = {}\n",
    "    additive = {}\n",
    "\n",
    "    for col_pair in itertools.combinations(relevant_features, 2):\n",
    "        interactions[f'{col_pair[0]}_times_{col_pair[1]}'] = data[col_pair[0]] * data[col_pair[1]]\n",
    "        ratios[f'{col_pair[0]}_div_{col_pair[1]}'] = data[col_pair[0]] / (data[col_pair[1]] + 1e-8)\n",
    "        differences[f'{col_pair[0]}_minus_{col_pair[1]}'] = data[col_pair[0]] - data[col_pair[1]]\n",
    "        additive[f'{col_pair[0]}_plus_{col_pair[1]}'] = data[col_pair[0]] + data[col_pair[1]]\n",
    "\n",
    "    for col in relevant_features:\n",
    "        self_interactions[f'{col}_squared'] = data[col] ** 2\n",
    "\n",
    "    # Creating lags for all relevant features\n",
    "    for col in relevant_features:\n",
    "        lags[f'{col}_lag1'] = data[col].shift(1)\n",
    "        lags[f'{col}_lag3'] = data[col].shift(3)\n",
    "\n",
    "    # Concatenate all new features with the original data\n",
    "    data = pd.concat([data, pd.DataFrame(interactions), pd.DataFrame(ratios),\n",
    "                      pd.DataFrame(differences), pd.DataFrame(lags), pd.DataFrame(self_interactions), pd.DataFrame(additive)], axis=1)\n",
    "\n",
    "    data['wind_magnitude'] = np.sqrt(data['wind_speed_u_10m:ms']**2 + data['wind_speed_v_10m:ms']**2)\n",
    "    data['wind_direction'] = np.arctan2(data['wind_speed_v_10m:ms'], data['wind_speed_u_10m:ms'])\n",
    "    data['solar_angle_impact'] = np.sin(np.radians(data['sun_elevation:d']))\n",
    "\n",
    "    return data\n",
    "\n",
    "def generate_solar_features_3(data):\n",
    "    relevant_features = [\n",
    "        'clear_sky_rad:W', 'clear_sky_energy_1h:J', 'direct_rad_1h:J', 'sun_elevation:d',\n",
    "        'direct_rad:W', 'diffuse_rad:W', 'sun_azimuth:d', 'diffuse_rad_1h:J',\n",
    "        'air_density_2m:kgm3', 'wind_speed_v_10m:ms', 'fresh_snow_24h:cm',\n",
    "        'relative_humidity_1000hPa:p', 'total_cloud_cover:p', 'effective_cloud_cover:p',\n",
    "        'cloud_base_agl:m', 'snow_water:kgm2', 't_1000hPa:K', 'is_in_shadow:idx', 'dew_point_2m:K',\n",
    "        'pressure_100m:hPa'\n",
    "    ]\n",
    "\n",
    "    interactions = {}\n",
    "    ratios = {}\n",
    "    differences = {}\n",
    "    lags = {}\n",
    "    self_interactions = {}\n",
    "    additive = {}\n",
    "\n",
    "    for col_pair in itertools.combinations(relevant_features, 2):\n",
    "        interactions[f'{col_pair[0]}_times_{col_pair[1]}'] = data[col_pair[0]] * data[col_pair[1]]\n",
    "        ratios[f'{col_pair[0]}_div_{col_pair[1]}'] = data[col_pair[0]] / (data[col_pair[1]] + 1e-8)\n",
    "        differences[f'{col_pair[0]}_minus_{col_pair[1]}'] = data[col_pair[0]] - data[col_pair[1]]\n",
    "        additive[f'{col_pair[0]}_plus_{col_pair[1]}'] = data[col_pair[0]] + data[col_pair[1]]\n",
    "\n",
    "    for col in relevant_features:\n",
    "        self_interactions[f'{col}_squared'] = data[col] ** 2\n",
    "\n",
    "    # Creating lags for all relevant features\n",
    "    for col in relevant_features:\n",
    "        lags[f'{col}_lag1'] = data[col].shift(1)\n",
    "        lags[f'{col}_lag3'] = data[col].shift(3)\n",
    "\n",
    "    # Concatenate all new features with the original data\n",
    "    data = pd.concat([data, pd.DataFrame(interactions), pd.DataFrame(ratios),\n",
    "                      pd.DataFrame(differences), pd.DataFrame(lags), pd.DataFrame(self_interactions), pd.DataFrame(additive)], axis=1)\n",
    "\n",
    "    data['wind_magnitude'] = np.sqrt(data['wind_speed_u_10m:ms']**2 + data['wind_speed_v_10m:ms']**2)\n",
    "    data['wind_direction'] = np.arctan2(data['wind_speed_v_10m:ms'], data['wind_speed_u_10m:ms'])\n",
    "    data['solar_angle_impact'] = np.sin(np.radians(data['sun_elevation:d']))\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69b0daa58b2ca350"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Location A"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0d0cfec59058bf2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_a = resample_to_hourly(X_test_estimated_a)\n",
    "\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'fresh_snow_12h:cm', 'snow_melt_10min:mm', 'elevation:m', 'prob_rime:p', 'dew_or_rime:idx', 'precip_5min:mm', 'rain_water:kgm2', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms'])\n",
    "X_test_estimated_a = X_test_estimated_a.drop(columns=['snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'fresh_snow_12h:cm', 'snow_melt_10min:mm', 'elevation:m', 'prob_rime:p', 'dew_or_rime:idx', 'precip_5min:mm', 'rain_water:kgm2', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75c9d9c45c502dad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols_to_impute = ['ceiling_height_agl:m', 'cloud_base_agl:m']\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "X_test_estimated_a[cols_to_impute] = imputer.fit_transform(X_test_estimated_a[cols_to_impute])\n",
    "df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "df = df.dropna(subset=['pv_measurement'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a09a0b32062fea94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = is_estimated(df)\n",
    "df = generate_solar_features_1(df)\n",
    "\n",
    "X_test_estimated_a = is_estimated(X_test_estimated_a, 'date_forecast')\n",
    "X_test_estimated_a = generate_solar_features_1(X_test_estimated_a)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3059f41994e870a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ed2038334240320"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4126ee5dde41ded5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4555af06c5f2080"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_importance = predictor.feature_importance(val_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8474dd3632bd353b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_features = feature_importance[feature_importance['importance'] > 0.1].index.tolist()\n",
    "\n",
    "X_train = X_train[best_features]\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "X_val = X_val[best_features]\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "label = 'pv_measurement'\n",
    "\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7aa3cdfb14557e57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f66d3785bf4dfd4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_estimated_a = X_test_estimated_a[best_features]\n",
    "\n",
    "y_pred = predictor.predict(X_test_estimated_a)\n",
    "y_pred = y_pred.clip(lower=0)\n",
    "y_pred = y_pred.reset_index(drop=True)\n",
    "y_pred.index.name = 'id'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4476725d1dec50de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df.to_csv('result_a.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75773f527c517a71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Location B"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d63f092e6c19781a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_b = resample_to_hourly(X_test_estimated_b)\n",
    "\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'fresh_snow_12h:cm', 'precip_5min:mm', 'rain_water:kgm2', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms'])\n",
    "X_test_estimated_b = X_test_estimated_b.drop(columns=['snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'fresh_snow_12h:cm', 'precip_5min:mm', 'rain_water:kgm2', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1acc56b05961f178"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols_to_impute = ['ceiling_height_agl:m', 'cloud_base_agl:m']\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "X_test_estimated_b[cols_to_impute] = imputer.fit_transform(X_test_estimated_b[cols_to_impute])\n",
    "df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "df = df.dropna(subset=['pv_measurement'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9c20a61e6b48bb0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segments = find_long_constant_periods(train_b['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "df = remove_unwanted_rows(df)\n",
    "df = lag_features_by_one_hour(df, ['diffuse_rad_1h:J', 'direct_rad_1h:J', 'clear_sky_energy_1h:J'])\n",
    "df = is_estimated(df)\n",
    "\n",
    "X_test_estimated_b = is_estimated(X_test_estimated_b, 'date_forecast')\n",
    "X_test_estimated_b = lag_features_by_one_hour(X_test_estimated_b, ['diffuse_rad_1h:J', 'direct_rad_1h:J', 'clear_sky_energy_1h:J'], 'date_forecast')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6bd19822434f10d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62f8b79f037c6c81"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='best_quality', num_gpus=1, num_stack_levels=0, use_bag_holdout=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e44be09a72527e3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5905f9a27a139b86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_importance = predictor.feature_importance(val_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1ea263f3a89cb73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(X_test_estimated_b)\n",
    "y_pred = y_pred.clip(lower=0)\n",
    "y_pred = y_pred.reset_index(drop=True)\n",
    "y_pred.index.name = 'id'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b43935d99b504a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df.to_csv('result_b.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e142b9f8cefbe38c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Location C"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efe19daf24baffe4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_c = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_c = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "X_train_observed_c = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_c, X_train_estimated_c])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_c = resample_to_hourly(X_test_estimated_c)\n",
    "\n",
    "df = pd.merge(df, train_c, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'fresh_snow_12h:cm', 'precip_5min:mm', 'rain_water:kgm2', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms'])\n",
    "X_test_estimated_c = X_test_estimated_c.drop(columns=['snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'fresh_snow_12h:cm', 'precip_5min:mm', 'rain_water:kgm2', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15ac503f444ec71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols_to_impute = ['ceiling_height_agl:m', 'cloud_base_agl:m']\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "X_test_estimated_c[cols_to_impute] = imputer.fit_transform(X_test_estimated_c[cols_to_impute])\n",
    "df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "df = df.dropna(subset=['pv_measurement'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e2725c42a9de7db"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segments = find_long_constant_periods(train_c['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "df = is_estimated(df)\n",
    "df = lag_features_by_one_hour(df, ['diffuse_rad_1h:J', 'direct_rad_1h:J', 'clear_sky_energy_1h:J'])\n",
    "\n",
    "X_test_estimated_c = is_estimated(X_test_estimated_c, 'date_forecast')\n",
    "X_test_estimated_c = lag_features_by_one_hour(X_test_estimated_c, ['diffuse_rad_1h:J', 'direct_rad_1h:J', 'clear_sky_energy_1h:J'], 'date_forecast')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c51a402158bb39d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f71b81bf06dbef5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='best_quality', num_gpus=1, num_stack_levels=0, use_bag_holdout=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86cc7e43cc035c43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f26f8ada4c9aea2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(X_test_estimated_c)\n",
    "y_pred = y_pred.clip(lower=0)\n",
    "y_pred = y_pred.reset_index(drop=True)\n",
    "y_pred.index.name = 'id'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2ef2440b44f9950"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df.to_csv('result_c.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce14d8c9b4f9fbe4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
