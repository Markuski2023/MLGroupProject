{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Team description\n",
    "\n",
    "Team: Ehhhhhhh\n",
    "\n",
    "Markus Kinn, 106660\n",
    "Mario Haroun, 543915\n",
    "Torstein Korten, 543955"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5598f9ea683b2936"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def remove_unwanted_rows(df):\n",
    "    unwanted_rows = (df['direct_rad:W'] == 0) & (df['diffuse_rad:W'] == 0) & (df['pv_measurement'] > 200) & (df['sun_elevation:d'] < 0) & (df['is_day:idx'] == 0)\n",
    "    cleaned_df = df[~unwanted_rows]\n",
    "    return cleaned_df\n",
    "def remove_highly_correlated_features(df, threshold):\n",
    "    # Compute the Pearson correlation matrix\n",
    "    correlation_matrix = df.corr(method='pearson')\n",
    "\n",
    "    # Initialize an empty list to hold features to be removed\n",
    "    features_to_remove = []\n",
    "\n",
    "    # Traverse the correlation matrix to find highly correlated features\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            feature1 = correlation_matrix.columns[i]\n",
    "            feature2 = correlation_matrix.columns[j]\n",
    "\n",
    "            # Check for high absolute correlation\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                # Add one of the features to the list if it's not already there\n",
    "                if feature1 not in features_to_remove and feature2 not in features_to_remove:\n",
    "                    features_to_remove.append(feature1)\n",
    "\n",
    "    # Drop the identified features from the DataFrame\n",
    "    filtered_df = df.drop(columns=features_to_remove)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def find_long_constant_periods(data, threshold):\n",
    "    start = None\n",
    "    segments = []\n",
    "    for i in range(1, len(data)):\n",
    "        if data[i] == data[i-1] and data[i] != 0:\n",
    "            if start is None:\n",
    "                start = i-1\n",
    "        else:\n",
    "            if start is not None:\n",
    "                if (i - start) > threshold:\n",
    "                    segments.append((start, i))\n",
    "                start = None\n",
    "    return segments\n",
    "\n",
    "def remove_constant_periods(df, segments):\n",
    "    drop_indices = []\n",
    "    for start, end in segments:\n",
    "        drop_indices.extend(range(start, end))\n",
    "    return df.drop(drop_indices)\n",
    "\n",
    "def lag_features_by_one_hour(df, column_names, time_col='time'):\n",
    "\n",
    "    # Check if the DataFrame has a time-based index\n",
    "    df['index'] = df[time_col]\n",
    "    df = df.set_index('index')\n",
    "\n",
    "    # Loop through each column name to create a lagged feature\n",
    "    for col in column_names:\n",
    "        lagged_col_name = f\"{col}\"\n",
    "        df[lagged_col_name] = df[col].shift(freq='-1H')\n",
    "\n",
    "    return df\n",
    "\n",
    "def is_estimated(df, time_col='time'):\n",
    "    split_date = '2022-10-27'\n",
    "    df['is_estimated'] = 0  # Initialize with 0 (indicating observed)\n",
    "    df.loc[df[time_col] >= pd.Timestamp(split_date), 'is_estimated'] = 1  # Set 1 for estimated data\n",
    "    return df\n",
    "\n",
    "def resample_to_hourly(df, datetime_column='date_forecast'):\n",
    "    df[datetime_column] = pd.to_datetime(df[datetime_column])\n",
    "    df.sort_values(by=datetime_column, inplace=True)\n",
    "\n",
    "    df.set_index(datetime_column, inplace=True)\n",
    "\n",
    "    df_hourly = df.resample('H').mean()\n",
    "\n",
    "    df_hourly.dropna(how='all', inplace=True)\n",
    "\n",
    "    df_hourly.reset_index(inplace=True)\n",
    "\n",
    "    return df_hourly\n",
    "\n",
    "def generate_solar_features_1(data):\n",
    "    relevant_features = [\n",
    "        'direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'sun_elevation:d', 'sun_azimuth:d',\n",
    "        'clear_sky_energy_1h:J', 'direct_rad_1h:J', 'effective_cloud_cover:p', 'diffuse_rad_1h:J',\n",
    "        'is_in_shadow:idx', 'total_cloud_cover:p', 'wind_speed_u_10m:ms', 'snow_water:kgm2',\n",
    "        'relative_humidity_1000hPa:p', 'is_day:idx', 'wind_speed_v_10m:ms', 'cloud_base_agl:m',\n",
    "        'fresh_snow_24h:cm', 'wind_speed_10m:ms', 'pressure_100m:hPa'\n",
    "    ]\n",
    "\n",
    "    interactions = {}\n",
    "    ratios = {}\n",
    "    differences = {}\n",
    "    lags = {}\n",
    "    self_interactions = {}\n",
    "    additive = {}\n",
    "\n",
    "    for col_pair in itertools.combinations(relevant_features, 2):\n",
    "        interactions[f'{col_pair[0]}_times_{col_pair[1]}'] = data[col_pair[0]] * data[col_pair[1]]\n",
    "        ratios[f'{col_pair[0]}_div_{col_pair[1]}'] = data[col_pair[0]] / (data[col_pair[1]] + 1e-8)\n",
    "        differences[f'{col_pair[0]}_minus_{col_pair[1]}'] = data[col_pair[0]] - data[col_pair[1]]\n",
    "        additive[f'{col_pair[0]}_plus_{col_pair[1]}'] = data[col_pair[0]] + data[col_pair[1]]\n",
    "\n",
    "    for col in relevant_features:\n",
    "        self_interactions[f'{col}_squared'] = data[col] ** 2\n",
    "\n",
    "    # Creating lags for all relevant features\n",
    "    for col in relevant_features:\n",
    "        lags[f'{col}_lag1'] = data[col].shift(1)\n",
    "        lags[f'{col}_lag3'] = data[col].shift(3)\n",
    "\n",
    "    # Concatenate all new features with the original data\n",
    "    data = pd.concat([data, pd.DataFrame(interactions), pd.DataFrame(ratios),\n",
    "                      pd.DataFrame(differences), pd.DataFrame(lags), pd.DataFrame(self_interactions), pd.DataFrame(additive)], axis=1)\n",
    "\n",
    "    data['wind_magnitude'] = np.sqrt(data['wind_speed_u_10m:ms']**2 + data['wind_speed_v_10m:ms']**2)\n",
    "    data['wind_direction'] = np.arctan2(data['wind_speed_v_10m:ms'], data['wind_speed_u_10m:ms'])\n",
    "    data['solar_angle_impact'] = np.sin(np.radians(data['sun_elevation:d']))\n",
    "\n",
    "    return data\n",
    "\n",
    "def generate_solar_features_2(data):\n",
    "    relevant_features = [\n",
    "        'sun_elevation:d', 'clear_sky_rad:W', 'direct_rad:W', 'diffuse_rad:W',\n",
    "        'sun_azimuth:d', 'clear_sky_energy_1h:J', 'cloud_base_agl:m', 'diffuse_rad_1h:J',\n",
    "        'effective_cloud_cover:p', 'direct_rad_1h:J', 'snow_water:kgm2', 'is_in_shadow:idx',\n",
    "        'fresh_snow_24h:cm', 'wind_speed_u_10m:ms', 'total_cloud_cover:p', 'msl_pressure:hPa',\n",
    "        'is_day:idx', 'relative_humidity_1000hPa:p', 'pressure_100m:hPa', 'ceiling_height_agl:m'\n",
    "    ]\n",
    "\n",
    "    interactions = {}\n",
    "    ratios = {}\n",
    "    differences = {}\n",
    "    lags = {}\n",
    "    self_interactions = {}\n",
    "    additive = {}\n",
    "\n",
    "    for col_pair in itertools.combinations(relevant_features, 2):\n",
    "        interactions[f'{col_pair[0]}_times_{col_pair[1]}'] = data[col_pair[0]] * data[col_pair[1]]\n",
    "        ratios[f'{col_pair[0]}_div_{col_pair[1]}'] = data[col_pair[0]] / (data[col_pair[1]] + 1e-8)\n",
    "        differences[f'{col_pair[0]}_minus_{col_pair[1]}'] = data[col_pair[0]] - data[col_pair[1]]\n",
    "        additive[f'{col_pair[0]}_plus_{col_pair[1]}'] = data[col_pair[0]] + data[col_pair[1]]\n",
    "\n",
    "    for col in relevant_features:\n",
    "        self_interactions[f'{col}_squared'] = data[col] ** 2\n",
    "\n",
    "    # Creating lags for all relevant features\n",
    "    for col in relevant_features:\n",
    "        lags[f'{col}_lag1'] = data[col].shift(1)\n",
    "        lags[f'{col}_lag3'] = data[col].shift(3)\n",
    "\n",
    "    # Concatenate all new features with the original data\n",
    "    data = pd.concat([data, pd.DataFrame(interactions), pd.DataFrame(ratios),\n",
    "                      pd.DataFrame(differences), pd.DataFrame(lags), pd.DataFrame(self_interactions), pd.DataFrame(additive)], axis=1)\n",
    "\n",
    "    data['wind_magnitude'] = np.sqrt(data['wind_speed_u_10m:ms']**2 + data['wind_speed_v_10m:ms']**2)\n",
    "    data['wind_direction'] = np.arctan2(data['wind_speed_v_10m:ms'], data['wind_speed_u_10m:ms'])\n",
    "    data['solar_angle_impact'] = np.sin(np.radians(data['sun_elevation:d']))\n",
    "\n",
    "    return data\n",
    "\n",
    "def generate_solar_features_3(data):\n",
    "    relevant_features = [\n",
    "        'clear_sky_rad:W', 'clear_sky_energy_1h:J', 'direct_rad_1h:J', 'sun_elevation:d',\n",
    "        'direct_rad:W', 'diffuse_rad:W', 'sun_azimuth:d', 'diffuse_rad_1h:J',\n",
    "        'air_density_2m:kgm3', 'wind_speed_v_10m:ms', 'fresh_snow_24h:cm',\n",
    "        'relative_humidity_1000hPa:p', 'total_cloud_cover:p', 'effective_cloud_cover:p',\n",
    "        'cloud_base_agl:m', 'snow_water:kgm2', 't_1000hPa:K', 'is_in_shadow:idx', 'dew_point_2m:K',\n",
    "        'pressure_100m:hPa'\n",
    "    ]\n",
    "\n",
    "    interactions = {}\n",
    "    ratios = {}\n",
    "    differences = {}\n",
    "    lags = {}\n",
    "    self_interactions = {}\n",
    "    additive = {}\n",
    "\n",
    "    for col_pair in itertools.combinations(relevant_features, 2):\n",
    "        interactions[f'{col_pair[0]}_times_{col_pair[1]}'] = data[col_pair[0]] * data[col_pair[1]]\n",
    "        ratios[f'{col_pair[0]}_div_{col_pair[1]}'] = data[col_pair[0]] / (data[col_pair[1]] + 1e-8)\n",
    "        differences[f'{col_pair[0]}_minus_{col_pair[1]}'] = data[col_pair[0]] - data[col_pair[1]]\n",
    "        additive[f'{col_pair[0]}_plus_{col_pair[1]}'] = data[col_pair[0]] + data[col_pair[1]]\n",
    "\n",
    "    for col in relevant_features:\n",
    "        self_interactions[f'{col}_squared'] = data[col] ** 2\n",
    "\n",
    "    # Creating lags for all relevant features\n",
    "    for col in relevant_features:\n",
    "        lags[f'{col}_lag1'] = data[col].shift(1)\n",
    "        lags[f'{col}_lag3'] = data[col].shift(3)\n",
    "\n",
    "    # Concatenate all new features with the original data\n",
    "    data = pd.concat([data, pd.DataFrame(interactions), pd.DataFrame(ratios),\n",
    "                      pd.DataFrame(differences), pd.DataFrame(lags), pd.DataFrame(self_interactions), pd.DataFrame(additive)], axis=1)\n",
    "\n",
    "    data['wind_magnitude'] = np.sqrt(data['wind_speed_u_10m:ms']**2 + data['wind_speed_v_10m:ms']**2)\n",
    "    data['wind_direction'] = np.arctan2(data['wind_speed_v_10m:ms'], data['wind_speed_u_10m:ms'])\n",
    "    data['solar_angle_impact'] = np.sin(np.radians(data['sun_elevation:d']))\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69b0daa58b2ca350"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Location A"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea6e7e723baaca39"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\Desktop\\NTNU\\ML\\Testing\\utils.py:58: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n",
      "C:\\Users\\marku\\Desktop\\NTNU\\ML\\Testing\\utils.py:58: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n"
     ]
    }
   ],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_a = resample_to_hourly(X_test_estimated_a)\n",
    "\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'fresh_snow_12h:cm', 'snow_melt_10min:mm', 'elevation:m', 'prob_rime:p', 'dew_or_rime:idx'])\n",
    "X_test_estimated_a = X_test_estimated_a.drop(columns=['snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'fresh_snow_12h:cm', 'snow_melt_10min:mm', 'elevation:m', 'prob_rime:p', 'dew_or_rime:idx'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90d55d6f2764ad60"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "cols_to_impute = ['ceiling_height_agl:m', 'cloud_base_agl:m']\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "X_test_estimated_a[cols_to_impute] = imputer.fit_transform(X_test_estimated_a[cols_to_impute])\n",
    "df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "df = df.dropna(subset=['pv_measurement'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6eb9c687a2edeb7"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df = is_estimated(df)\n",
    "df = generate_solar_features_1(df)\n",
    "\n",
    "X_test_estimated_a = is_estimated(X_test_estimated_a, 'date_forecast')\n",
    "X_test_estimated_a = generate_solar_features_1(X_test_estimated_a)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77de508871f2f95a"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd8baaf46aeee5f1"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231112_140358\\\"\n",
      "Presets specified: ['medium_quality']\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (31863 samples, 109.86 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231112_140358\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Disk Space Avail:   900.21 GB / 2047.46 GB (44.0%)\n",
      "Train Data Rows:    31863\n",
      "Train Data Columns: 859\n",
      "Tuning Data Rows:    2197\n",
      "Tuning Data Columns: 859\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 649.75117, 1177.67732)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    46458.04 MB\n",
      "\tTrain Data (Original)  Memory Usage: 117.17 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 858 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', [])   :   1 | ['is_estimated']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 858 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool']) :   1 | ['is_estimated']\n",
      "\t2.4s = Fit runtime\n",
      "\t859 features in original data used to generate 859 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 116.93 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-152.7299\t = Validation score   (-mean_absolute_error)\n",
      "\t0.69s\t = Training   runtime\n",
      "\t0.59s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-153.3907\t = Validation score   (-mean_absolute_error)\n",
      "\t0.71s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 94.0947\n",
      "[2000]\tvalid_set's l1: 90.9007\n",
      "[3000]\tvalid_set's l1: 89.7331\n",
      "[4000]\tvalid_set's l1: 89.1377\n",
      "[5000]\tvalid_set's l1: 88.8309\n",
      "[6000]\tvalid_set's l1: 88.7296\n",
      "[7000]\tvalid_set's l1: 88.6401\n",
      "[8000]\tvalid_set's l1: 88.5747\n",
      "[9000]\tvalid_set's l1: 88.4477\n",
      "[10000]\tvalid_set's l1: 88.3347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-88.3335\t = Validation score   (-mean_absolute_error)\n",
      "\t106.06s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 95.1734\n",
      "[2000]\tvalid_set's l1: 93.2164\n",
      "[3000]\tvalid_set's l1: 92.7358\n",
      "[4000]\tvalid_set's l1: 92.5886\n",
      "[5000]\tvalid_set's l1: 92.4035\n",
      "[6000]\tvalid_set's l1: 92.2893\n",
      "[7000]\tvalid_set's l1: 92.2514\n",
      "[8000]\tvalid_set's l1: 92.2207\n",
      "[9000]\tvalid_set's l1: 92.2271\n",
      "[10000]\tvalid_set's l1: 92.2203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-92.2191\t = Validation score   (-mean_absolute_error)\n",
      "\t157.85s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-106.3874\t = Validation score   (-mean_absolute_error)\n",
      "\t548.28s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-95.4077\t = Validation score   (-mean_absolute_error)\n",
      "\t647.9s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-104.7218\t = Validation score   (-mean_absolute_error)\n",
      "\t95.41s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-107.9299\t = Validation score   (-mean_absolute_error)\n",
      "\t46.1s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-99.4111\t = Validation score   (-mean_absolute_error)\n",
      "\t32.18s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-89.0747\t = Validation score   (-mean_absolute_error)\n",
      "\t103.73s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 94.7572\n",
      "[2000]\tvalid_set's l1: 94.1652\n",
      "[3000]\tvalid_set's l1: 94.105\n",
      "[4000]\tvalid_set's l1: 94.0865\n",
      "[5000]\tvalid_set's l1: 94.08\n",
      "[6000]\tvalid_set's l1: 94.0778\n",
      "[7000]\tvalid_set's l1: 94.0771\n",
      "[8000]\tvalid_set's l1: 94.0768\n",
      "[9000]\tvalid_set's l1: 94.0767\n",
      "[10000]\tvalid_set's l1: 94.0767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-94.0767\t = Validation score   (-mean_absolute_error)\n",
      "\t475.5s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-84.527\t = Validation score   (-mean_absolute_error)\n",
      "\t0.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2224.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231112_140358\\\")\n"
     ]
    }
   ],
   "source": [
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53082f369607b234"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                  model   score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2  -84.526975       0.618043  367.825929                0.000000           0.195613            2       True         12\n",
      "1            LightGBMXT  -88.333488       0.116524  106.057042                0.116524         106.057042            1       True          3\n",
      "2        NeuralNetTorch  -89.074721       0.395000  103.727405                0.395000         103.727405            1       True         10\n",
      "3              LightGBM  -92.219139       0.106518  157.845870                0.106518         157.845870            1       True          4\n",
      "4         LightGBMLarge  -94.076696       0.310780  475.501889                0.310780         475.501889            1       True         11\n",
      "5              CatBoost  -95.407715       0.039000  647.904157                0.039000         647.904157            1       True          6\n",
      "6               XGBoost  -99.411094       0.063796   32.181466                0.063796          32.181466            1       True          9\n",
      "7         ExtraTreesMSE -104.721836       0.077999   95.407578                0.077999          95.407578            1       True          7\n",
      "8       RandomForestMSE -106.387434       0.061289  548.283979                0.061289         548.283979            1       True          5\n",
      "9       NeuralNetFastAI -107.929871       0.056002   46.103105                0.056002          46.103105            1       True          8\n",
      "10       KNeighborsUnif -152.729857       0.589772    0.693891                0.589772           0.693891            1       True          1\n",
      "11       KNeighborsDist -153.390743       0.561117    0.707905                0.561117           0.707905            1       True          2\n",
      "Number of models trained: 12\n",
      "Types of models trained:\n",
      "{'XTModel', 'RFModel', 'LGBModel', 'CatBoostModel', 'TabularNeuralNetTorchModel', 'KNNModel', 'WeightedEnsembleModel', 'XGBoostModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])     : 858 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "('int', ['bool']) :   1 | ['is_estimated']\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\Desktop\\Envs\\MLenv\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:169: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    }
   ],
   "source": [
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "505631e257286d8e"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 859 features using 2197 rows with 5 shuffle sets...\n",
      "\t3392.82s\t= Expected runtime (678.56s per shuffle set)\n",
      "\t1696.77s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    }
   ],
   "source": [
    "feature_importance = predictor.feature_importance(val_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15e0b7c735fa9ba9"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231112_150921\\\"\n",
      "Presets specified: ['medium_quality']\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (31863 samples, 67.68 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231112_150921\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Disk Space Avail:   898.92 GB / 2047.46 GB (43.9%)\n",
      "Train Data Rows:    31863\n",
      "Train Data Columns: 529\n",
      "Tuning Data Rows:    2197\n",
      "Tuning Data Columns: 529\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 649.75117, 1177.67732)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    44918.18 MB\n",
      "\tTrain Data (Original)  Memory Usage: 72.07 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 529 | ['direct_rad:W_plus_diffuse_rad:W', 'direct_rad:W_times_sun_elevation:d', 'direct_rad:W_times_clear_sky_rad:W', 'direct_rad:W_plus_sun_elevation:d', 'direct_rad:W_plus_clear_sky_rad:W', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 529 | ['direct_rad:W_plus_diffuse_rad:W', 'direct_rad:W_times_sun_elevation:d', 'direct_rad:W_times_clear_sky_rad:W', 'direct_rad:W_plus_sun_elevation:d', 'direct_rad:W_plus_clear_sky_rad:W', ...]\n",
      "\t1.2s = Fit runtime\n",
      "\t529 features in original data used to generate 529 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 72.07 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.25s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-156.9257\t = Validation score   (-mean_absolute_error)\n",
      "\t0.38s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-157.3448\t = Validation score   (-mean_absolute_error)\n",
      "\t0.35s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 94.8094\n",
      "[2000]\tvalid_set's l1: 91.1564\n",
      "[3000]\tvalid_set's l1: 89.6507\n",
      "[4000]\tvalid_set's l1: 89.0496\n",
      "[5000]\tvalid_set's l1: 88.6239\n",
      "[6000]\tvalid_set's l1: 88.4249\n",
      "[7000]\tvalid_set's l1: 88.142\n",
      "[8000]\tvalid_set's l1: 87.9455\n",
      "[9000]\tvalid_set's l1: 87.8285\n",
      "[10000]\tvalid_set's l1: 87.6959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-87.6948\t = Validation score   (-mean_absolute_error)\n",
      "\t71.69s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 93.4136\n",
      "[2000]\tvalid_set's l1: 92.0652\n",
      "[3000]\tvalid_set's l1: 91.7453\n",
      "[4000]\tvalid_set's l1: 91.4531\n",
      "[5000]\tvalid_set's l1: 91.3031\n",
      "[6000]\tvalid_set's l1: 91.2216\n",
      "[7000]\tvalid_set's l1: 91.174\n",
      "[8000]\tvalid_set's l1: 91.1615\n",
      "[9000]\tvalid_set's l1: 91.1432\n",
      "[10000]\tvalid_set's l1: 91.1323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-91.1321\t = Validation score   (-mean_absolute_error)\n",
      "\t99.92s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-105.971\t = Validation score   (-mean_absolute_error)\n",
      "\t334.66s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-92.205\t = Validation score   (-mean_absolute_error)\n",
      "\t467.95s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-104.6491\t = Validation score   (-mean_absolute_error)\n",
      "\t51.87s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-111.421\t = Validation score   (-mean_absolute_error)\n",
      "\t37.23s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-94.3357\t = Validation score   (-mean_absolute_error)\n",
      "\t455.41s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-92.247\t = Validation score   (-mean_absolute_error)\n",
      "\t85.75s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 94.1835\n",
      "[2000]\tvalid_set's l1: 93.418\n",
      "[3000]\tvalid_set's l1: 93.2886\n",
      "[4000]\tvalid_set's l1: 93.2495\n",
      "[5000]\tvalid_set's l1: 93.2393\n",
      "[6000]\tvalid_set's l1: 93.2345\n",
      "[7000]\tvalid_set's l1: 93.2334\n",
      "[8000]\tvalid_set's l1: 93.233\n",
      "[9000]\tvalid_set's l1: 93.2328\n",
      "[10000]\tvalid_set's l1: 93.2328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-93.2328\t = Validation score   (-mean_absolute_error)\n",
      "\t302.54s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-85.379\t = Validation score   (-mean_absolute_error)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1915.25s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231112_150921\\\")\n"
     ]
    }
   ],
   "source": [
    "best_features = feature_importance[feature_importance['importance'] > 0.1].index.tolist()\n",
    "\n",
    "X_train = X_train[best_features]\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "X_val = X_val[best_features]\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "label = 'pv_measurement'\n",
    "\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f9899656927c481"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                  model   score_val  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2  -85.379029       0.687539  1180.933998                0.000000           0.214641            2       True         12\n",
      "1            LightGBMXT  -87.694764       0.110260    71.693029                0.110260          71.693029            1       True          3\n",
      "2              LightGBM  -91.132115       0.123772    99.917832                0.123772          99.917832            1       True          4\n",
      "3              CatBoost  -92.205027       0.022024   467.951797                0.022024         467.951797            1       True          6\n",
      "4        NeuralNetTorch  -92.247037       0.254052    85.750385                0.254052          85.750385            1       True         10\n",
      "5         LightGBMLarge  -93.232782       0.335674   302.541405                0.335674         302.541405            1       True         11\n",
      "6               XGBoost  -94.335717       0.177431   455.406314                0.177431         455.406314            1       True          9\n",
      "7         ExtraTreesMSE -104.649052       0.061472    51.867387                0.061472          51.867387            1       True          7\n",
      "8       RandomForestMSE -105.970954       0.061368   334.657710                0.061368         334.657710            1       True          5\n",
      "9       NeuralNetFastAI -111.421040       0.036097    37.228828                0.036097          37.228828            1       True          8\n",
      "10       KNeighborsUnif -156.925679       0.324810     0.382961                0.324810           0.382961            1       True          1\n",
      "11       KNeighborsDist -157.344813       0.360838     0.353598                0.360838           0.353598            1       True          2\n",
      "Number of models trained: 12\n",
      "Types of models trained:\n",
      "{'XTModel', 'RFModel', 'LGBModel', 'CatBoostModel', 'TabularNeuralNetTorchModel', 'KNNModel', 'WeightedEnsembleModel', 'XGBoostModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', []) : 529 | ['direct_rad:W_plus_diffuse_rad:W', 'direct_rad:W_times_sun_elevation:d', 'direct_rad:W_times_clear_sky_rad:W', 'direct_rad:W_plus_sun_elevation:d', 'direct_rad:W_plus_clear_sky_rad:W', ...]\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\Desktop\\Envs\\MLenv\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:169: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    }
   ],
   "source": [
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "704ba821db961be9"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "X_test_estimated_a = X_test_estimated_a[best_features]\n",
    "\n",
    "y_pred = predictor.predict(X_test_estimated_a)\n",
    "y_pred = y_pred.clip(lower=0)\n",
    "y_pred = y_pred.reset_index(drop=True)\n",
    "y_pred.index.name = 'id'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcfeb6f50df56f61"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df.to_csv('result_a.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5346f91385fe9ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Location B"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "131311f38b8e66b6"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\Desktop\\NTNU\\ML\\Testing\\utils.py:58: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n",
      "C:\\Users\\marku\\Desktop\\NTNU\\ML\\Testing\\utils.py:58: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n"
     ]
    }
   ],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_b = resample_to_hourly(X_test_estimated_b)\n",
    "\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'snow_melt_10min:mm', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'fresh_snow_12h:cm', 'precip_5min:mm', 'rain_water:kgm2', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms', 'cloud_base_agl:m'])\n",
    "X_test_estimated_b = X_test_estimated_b.drop(columns=['snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'snow_melt_10min:mm', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'fresh_snow_12h:cm', 'precip_5min:mm', 'rain_water:kgm2', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "332cda705db35dea"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=123)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6919f9568b6aa3a5"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "segments = find_long_constant_periods(train_b['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "df = remove_unwanted_rows(df)\n",
    "df = is_estimated(df)\n",
    "df = lag_features_by_one_hour(df, ['diffuse_rad_1h:J', 'direct_rad_1h:J', 'clear_sky_energy_1h:J'])\n",
    "\n",
    "X_test_estimated_b = is_estimated(X_test_estimated_b, 'date_forecast')\n",
    "X_test_estimated_b = lag_features_by_one_hour(X_test_estimated_b, ['diffuse_rad_1h:J', 'direct_rad_1h:J', 'clear_sky_energy_1h:J'], 'date_forecast')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2421ee4f0037a25e"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7739ea08a77f7ec"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231112_154122\\\"\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231112_154122\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Disk Space Avail:   897.60 GB / 2047.46 GB (43.8%)\n",
      "Train Data Rows:    27807\n",
      "Train Data Columns: 34\n",
      "Tuning Data Rows:    1801\n",
      "Tuning Data Columns: 34\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (1152.3, -0.0, 96.61183, 205.14064)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    44674.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 8.05 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 33 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', [])   :  1 | ['is_estimated']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 33 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['is_estimated']\n",
      "\t0.1s = Fit runtime\n",
      "\t34 features in original data used to generate 34 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.85 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-19.8649\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-19.8219\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 12.5085\n",
      "[2000]\tvalid_set's l1: 12.2277\n",
      "[3000]\tvalid_set's l1: 12.0675\n",
      "[4000]\tvalid_set's l1: 12.0195\n",
      "[5000]\tvalid_set's l1: 11.9903\n",
      "[6000]\tvalid_set's l1: 12.0009\n",
      "[7000]\tvalid_set's l1: 11.9943\n",
      "[8000]\tvalid_set's l1: 11.9742\n",
      "[9000]\tvalid_set's l1: 11.9374\n",
      "[10000]\tvalid_set's l1: 11.9382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-11.9253\t = Validation score   (-mean_absolute_error)\n",
      "\t16.2s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 12.9825\n",
      "[2000]\tvalid_set's l1: 12.8098\n",
      "[3000]\tvalid_set's l1: 12.7336\n",
      "[4000]\tvalid_set's l1: 12.7099\n",
      "[5000]\tvalid_set's l1: 12.7158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-12.7029\t = Validation score   (-mean_absolute_error)\n",
      "\t9.36s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-14.2155\t = Validation score   (-mean_absolute_error)\n",
      "\t12.83s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-13.0591\t = Validation score   (-mean_absolute_error)\n",
      "\t36.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-13.7027\t = Validation score   (-mean_absolute_error)\n",
      "\t2.03s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-13.4804\t = Validation score   (-mean_absolute_error)\n",
      "\t21.75s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-13.5175\t = Validation score   (-mean_absolute_error)\n",
      "\t1.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-10.9905\t = Validation score   (-mean_absolute_error)\n",
      "\t80.35s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 12.0577\n",
      "[2000]\tvalid_set's l1: 12.0334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-12.0183\t = Validation score   (-mean_absolute_error)\n",
      "\t8.91s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-10.3098\t = Validation score   (-mean_absolute_error)\n",
      "\t0.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 192.3s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231112_154122\\\")\n"
     ]
    }
   ],
   "source": [
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d73c4dd2f951a26"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                  model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2 -10.309786       0.200825  136.763167                0.000000           0.182827            2       True         12\n",
      "1        NeuralNetTorch -10.990536       0.017999   80.354894                0.017999          80.354894            1       True         10\n",
      "2            LightGBMXT -11.925316       0.094237   16.201539                0.094237          16.201539            1       True          3\n",
      "3         LightGBMLarge -12.018280       0.025002    8.908605                0.025002           8.908605            1       True         11\n",
      "4              LightGBM -12.702907       0.039589    9.362858                0.039589           9.362858            1       True          4\n",
      "5              CatBoost -13.059088       0.005616   36.112140                0.005616          36.112140            1       True          6\n",
      "6       NeuralNetFastAI -13.480434       0.023998   21.752445                0.023998          21.752445            1       True          8\n",
      "7               XGBoost -13.517527       0.005999    1.157278                0.005999           1.157278            1       True          9\n",
      "8         ExtraTreesMSE -13.702736       0.062312    2.031413                0.062312           2.031413            1       True          7\n",
      "9       RandomForestMSE -14.215458       0.047896   12.827805                0.047896          12.827805            1       True          5\n",
      "10       KNeighborsDist -19.821922       0.041229    0.044588                0.041229           0.044588            1       True          2\n",
      "11       KNeighborsUnif -19.864934       0.098713    0.038374                0.098713           0.038374            1       True          1\n",
      "Number of models trained: 12\n",
      "Types of models trained:\n",
      "{'XTModel', 'RFModel', 'LGBModel', 'CatBoostModel', 'TabularNeuralNetTorchModel', 'KNNModel', 'WeightedEnsembleModel', 'XGBoostModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])     : 33 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "('int', ['bool']) :  1 | ['is_estimated']\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\Desktop\\Envs\\MLenv\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:169: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    }
   ],
   "source": [
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e64464c5b978c08a"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(X_test_estimated_b)\n",
    "y_pred = y_pred.clip(lower=0)\n",
    "y_pred = y_pred.reset_index(drop=True)\n",
    "y_pred.index.name = 'id'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21eaccd47b1a0212"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df.to_csv('result_b.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0107d8dfdaed75e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Location C"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "716f6d6ea95eafd2"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\Desktop\\NTNU\\ML\\Testing\\utils.py:58: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n",
      "C:\\Users\\marku\\Desktop\\NTNU\\ML\\Testing\\utils.py:58: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n"
     ]
    }
   ],
   "source": [
    "train_c = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_c = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "X_train_observed_c = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_c, X_train_estimated_c])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_c = resample_to_hourly(X_test_estimated_c)\n",
    "\n",
    "df = pd.merge(df, train_c, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'fresh_snow_12h:cm', 'precip_5min:mm', 'rain_water:kgm2', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms'])\n",
    "X_test_estimated_c = X_test_estimated_c.drop(columns=['snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'fresh_snow_12h:cm', 'precip_5min:mm', 'rain_water:kgm2', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e39b98e09cb81461"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "cols_to_impute = ['ceiling_height_agl:m', 'cloud_base_agl:m']\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "X_test_estimated_c[cols_to_impute] = imputer.fit_transform(X_test_estimated_c[cols_to_impute])\n",
    "df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "df = df.dropna(subset=['pv_measurement'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0accab8ac133f12"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "segments = find_long_constant_periods(train_c['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "df = is_estimated(df)\n",
    "df = lag_features_by_one_hour(df, ['diffuse_rad_1h:J', 'direct_rad_1h:J', 'clear_sky_energy_1h:J'])\n",
    "\n",
    "X_test_estimated_c = is_estimated(X_test_estimated_c, 'date_forecast')\n",
    "X_test_estimated_c = lag_features_by_one_hour(X_test_estimated_c, ['diffuse_rad_1h:J', 'direct_rad_1h:J', 'clear_sky_energy_1h:J'], 'date_forecast')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6729f3f01be55467"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2c4a11954688c09"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231112_154436\\\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231112_154436\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Disk Space Avail:   896.96 GB / 2047.46 GB (43.8%)\n",
      "Train Data Rows:    24600\n",
      "Train Data Columns: 35\n",
      "Tuning Data Rows:    1465\n",
      "Tuning Data Columns: 35\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
      "\tLabel info (max, min, mean, stddev): (999.6, 0.0, 79.84979, 168.49708)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    44973.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 3.75 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 34 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', [])   :  1 | ['is_estimated']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 34 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['is_estimated']\n",
      "\t0.1s = Fit runtime\n",
      "\t35 features in original data used to generate 35 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.57 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\t-27.8225\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\t-27.8009\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.2615\t = Validation score   (-mean_absolute_error)\n",
      "\t418.49s\t = Training   runtime\n",
      "\t8.38s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-12.611\t = Validation score   (-mean_absolute_error)\n",
      "\t389.09s\t = Training   runtime\n",
      "\t10.93s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ...\n",
      "\t-17.3421\t = Validation score   (-mean_absolute_error)\n",
      "\t8.6s\t = Training   runtime\n",
      "\t0.59s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.287\t = Validation score   (-mean_absolute_error)\n",
      "\t286.31s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
      "\t-16.3821\t = Validation score   (-mean_absolute_error)\n",
      "\t1.59s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.4644\t = Validation score   (-mean_absolute_error)\n",
      "\t95.07s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.3687\t = Validation score   (-mean_absolute_error)\n",
      "\t340.11s\t = Training   runtime\n",
      "\t1.44s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.0244\t = Validation score   (-mean_absolute_error)\n",
      "\t472.09s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-12.5091\t = Validation score   (-mean_absolute_error)\n",
      "\t1613.1s\t = Training   runtime\n",
      "\t13.96s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-11.1224\t = Validation score   (-mean_absolute_error)\n",
      "\t0.19s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3662.06s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231112_154436\\\")\n"
     ]
    }
   ],
   "source": [
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='best_quality', num_gpus=1, num_stack_levels=0, use_bag_holdout=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "524e6078b39a4263"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                     model  score_val  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L2 -11.122399       8.530623   890.767996                0.001000           0.185380            2       True         12\n",
      "1        LightGBMXT_BAG_L1 -11.261465       8.384051   418.490465                8.384051         418.490465            1       True          3\n",
      "2     LightGBMLarge_BAG_L1 -12.509137      13.955961  1613.097727               13.955961        1613.097727            1       True         11\n",
      "3          LightGBM_BAG_L1 -12.611008      10.929166   389.093676               10.929166         389.093676            1       True          4\n",
      "4    NeuralNetTorch_BAG_L1 -13.024384       0.145572   472.092152                0.145572         472.092152            1       True         10\n",
      "5           XGBoost_BAG_L1 -13.368729       1.441644   340.109518                1.441644         340.109518            1       True          9\n",
      "6   NeuralNetFastAI_BAG_L1 -13.464435       0.267771    95.070021                0.267771          95.070021            1       True          8\n",
      "7          CatBoost_BAG_L1 -15.286995       0.039102   286.307110                0.039102         286.307110            1       True          6\n",
      "8     ExtraTreesMSE_BAG_L1 -16.382144       0.746880     1.588033                0.746880           1.588033            1       True          7\n",
      "9   RandomForestMSE_BAG_L1 -17.342138       0.589404     8.598806                0.589404           8.598806            1       True          5\n",
      "10   KNeighborsDist_BAG_L1 -27.800896       0.372740     0.038998                0.372740           0.038998            1       True          2\n",
      "11   KNeighborsUnif_BAG_L1 -27.822523       0.480300     0.036000                0.480300           0.036000            1       True          1\n",
      "Number of models trained: 12\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_CatBoost', 'StackerEnsembleModel_KNN', 'StackerEnsembleModel_NNFastAiTabular', 'StackerEnsembleModel_RF', 'StackerEnsembleModel_XT', 'WeightedEnsembleModel', 'StackerEnsembleModel_XGBoost', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_TabularNeuralNetTorch'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])     : 34 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "('int', ['bool']) :  1 | ['is_estimated']\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\Desktop\\Envs\\MLenv\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:169: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    }
   ],
   "source": [
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be9ae122459ef7a7"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(X_test_estimated_c)\n",
    "y_pred = y_pred.clip(lower=0)\n",
    "y_pred = y_pred.reset_index(drop=True)\n",
    "y_pred.index.name = 'id'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "568445bcda6980a6"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df.to_csv('result_c.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e0db76ea4a67e10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_a = pd.read_csv('result_a.csv')\n",
    "df_b = pd.read_csv('result_b.csv')\n",
    "df_c = pd.read_csv('result_c.csv')\n",
    "\n",
    "def combine_dataframes(df1, df2, df3):\n",
    "    # Concatenate the dataframes in the specified order\n",
    "    combined = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "    # Rename the 'index' column to 'id'\n",
    "    combined.rename(columns={'index': 'id'}, inplace=True)\n",
    "\n",
    "    # Ensure the 'id' values range from 0 to 'x'\n",
    "    combined['id'] = range(len(combined))\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "df = combine_dataframes(df_a, df_b, df_c)\n",
    "df.to_csv('result.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "778f7126f95136f5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
