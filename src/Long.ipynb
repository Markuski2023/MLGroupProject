{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:02:20.585541300Z",
     "start_time": "2023-11-11T14:02:11.159246700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost\n",
    "from sklearn.impute import SimpleImputer\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "def remove_unwanted_rows(df):\n",
    "    unwanted_rows = (df['direct_rad:W'] == 0) & (df['diffuse_rad:W'] == 0) & (df['pv_measurement'] > 200) & (df['sun_elevation:d'] < 0) & (df['is_day:idx'] == 0)\n",
    "    cleaned_df = df[~unwanted_rows]\n",
    "    return cleaned_df\n",
    "def remove_highly_correlated_features(df, threshold):\n",
    "    # Compute the Pearson correlation matrix\n",
    "    correlation_matrix = df.corr(method='pearson')\n",
    "\n",
    "    # Initialize an empty list to hold features to be removed\n",
    "    features_to_remove = []\n",
    "\n",
    "    # Traverse the correlation matrix to find highly correlated features\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            feature1 = correlation_matrix.columns[i]\n",
    "            feature2 = correlation_matrix.columns[j]\n",
    "\n",
    "            # Check for high absolute correlation\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                # Add one of the features to the list if it's not already there\n",
    "                if feature1 not in features_to_remove and feature2 not in features_to_remove:\n",
    "                    features_to_remove.append(feature1)\n",
    "\n",
    "    # Drop the identified features from the DataFrame\n",
    "    filtered_df = df.drop(columns=features_to_remove)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def split_df_on_date(df, date_as_string):\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    first_df = df[df['time'] < date_as_string]\n",
    "    second_df = df[df['time'] >= date_as_string]\n",
    "\n",
    "    return first_df, second_df\n",
    "\n",
    "def split_df_on_ratio(df, ratio, random=False):\n",
    "    if random:\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "    split_index = int(len(df) * ratio)\n",
    "    df1 = df.iloc[:split_index]\n",
    "    df2 = df.iloc[split_index:]\n",
    "\n",
    "    return df1, df2\n",
    "\n",
    "def find_long_constant_periods(data, threshold):\n",
    "    start = None\n",
    "    segments = []\n",
    "    for i in range(1, len(data)):\n",
    "        if data[i] == data[i-1] and data[i] != 0:\n",
    "            if start is None:\n",
    "                start = i-1\n",
    "        else:\n",
    "            if start is not None:\n",
    "                if (i - start) > threshold:\n",
    "                    segments.append((start, i))\n",
    "                start = None\n",
    "    return segments\n",
    "\n",
    "def remove_constant_periods(df, segments):\n",
    "    drop_indices = []\n",
    "    for start, end in segments:\n",
    "        drop_indices.extend(range(start, end))\n",
    "    return df.drop(drop_indices)\n",
    "\n",
    "def lag_features_by_one_hour(df, column_names, time_col='time'):\n",
    "\n",
    "    # Check if the DataFrame has a time-based index\n",
    "    df['index'] = df[time_col]\n",
    "    df = df.set_index('index')\n",
    "\n",
    "    # Loop through each column name to create a lagged feature\n",
    "    for col in column_names:\n",
    "        lagged_col_name = f\"{col}\"\n",
    "        df[lagged_col_name] = df[col].shift(freq='-1H')\n",
    "\n",
    "    return df\n",
    "\n",
    "def is_estimated(df, time_col='time'):\n",
    "    split_date = '2022-10-27'\n",
    "    df['is_estimated'] = 0  # Initialize with 0 (indicating observed)\n",
    "    df.loc[df[time_col] >= pd.Timestamp(split_date), 'is_estimated'] = 1  # Set 1 for estimated data\n",
    "    return df\n",
    "\n",
    "def resample_to_hourly(df, datetime_column='date_forecast'):\n",
    "    df[datetime_column] = pd.to_datetime(df[datetime_column])\n",
    "    df.sort_values(by=datetime_column, inplace=True)\n",
    "\n",
    "    df.set_index(datetime_column, inplace=True)\n",
    "\n",
    "    df_hourly = df.resample('H').mean()\n",
    "\n",
    "    df_hourly.dropna(how='all', inplace=True)\n",
    "\n",
    "    df_hourly.reset_index(inplace=True)\n",
    "\n",
    "    return df_hourly\n",
    "\n",
    "def generate_wind_features(data):\n",
    "    data['wind_magnitude'] = np.sqrt(data['wind_speed_u_10m:ms']**2 + data['wind_speed_v_10m:ms']**2)\n",
    "    data['wind_direction'] = np.arctan2(data['wind_speed_v_10m:ms'], data['wind_speed_u_10m:ms'])\n",
    "    data['solar_angle_impact'] = np.sin(np.radians(data['sun_elevation:d']))\n",
    "\n",
    "    return data\n",
    "\n",
    "def generate_solar_features_1(data):\n",
    "    # These features we got by running AutoGluon feature importance on the original dataset and choosing the top 20\n",
    "    relevant_features = [\n",
    "        'direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'sun_elevation:d', 'sun_azimuth:d',\n",
    "        'clear_sky_energy_1h:J', 'direct_rad_1h:J', 'effective_cloud_cover:p', 'diffuse_rad_1h:J',\n",
    "        'is_in_shadow:idx', 'total_cloud_cover:p', 'wind_speed_u_10m:ms', 'snow_water:kgm2',\n",
    "        'relative_humidity_1000hPa:p', 'is_day:idx', 'wind_speed_v_10m:ms', 'cloud_base_agl:m',\n",
    "        'fresh_snow_24h:cm', 'wind_speed_10m:ms', 'pressure_100m:hPa'\n",
    "    ]\n",
    "\n",
    "    interactions = {}\n",
    "    ratios = {}\n",
    "    differences = {}\n",
    "    lags = {}\n",
    "    self_interactions = {}\n",
    "    additive = {}\n",
    "\n",
    "    for col_pair in itertools.combinations(relevant_features, 2):\n",
    "        interactions[f'{col_pair[0]}_times_{col_pair[1]}'] = data[col_pair[0]] * data[col_pair[1]]\n",
    "        ratios[f'{col_pair[0]}_div_{col_pair[1]}'] = data[col_pair[0]] / (data[col_pair[1]] + 1e-8)\n",
    "        differences[f'{col_pair[0]}_minus_{col_pair[1]}'] = data[col_pair[0]] - data[col_pair[1]]\n",
    "        additive[f'{col_pair[0]}_plus_{col_pair[1]}'] = data[col_pair[0]] + data[col_pair[1]]\n",
    "\n",
    "    for col in relevant_features:\n",
    "        self_interactions[f'{col}_squared'] = data[col] ** 2\n",
    "\n",
    "    # Creating lags for all relevant features\n",
    "    for col in relevant_features:\n",
    "        lags[f'{col}_lag1'] = data[col].shift(1)\n",
    "        lags[f'{col}_lag3'] = data[col].shift(3)\n",
    "\n",
    "    # Concatenate all new features with the original data\n",
    "    data = pd.concat([data, pd.DataFrame(interactions), pd.DataFrame(ratios),\n",
    "                      pd.DataFrame(differences), pd.DataFrame(lags), pd.DataFrame(self_interactions), pd.DataFrame(additive)], axis=1)\n",
    "\n",
    "    data['wind_magnitude'] = np.sqrt(data['wind_speed_u_10m:ms']**2 + data['wind_speed_v_10m:ms']**2)\n",
    "    data['wind_direction'] = np.arctan2(data['wind_speed_v_10m:ms'], data['wind_speed_u_10m:ms'])\n",
    "    data['solar_angle_impact'] = np.sin(np.radians(data['sun_elevation:d']))\n",
    "\n",
    "    return data\n",
    "\n",
    "def generate_solar_features_2(data):\n",
    "    # These features we got by running AutoGluon feature importance on the original dataset and choosing the top 20\n",
    "    relevant_features = [\n",
    "        'sun_elevation:d', 'clear_sky_rad:W', 'direct_rad:W', 'diffuse_rad:W',\n",
    "        'sun_azimuth:d', 'clear_sky_energy_1h:J', 'cloud_base_agl:m', 'diffuse_rad_1h:J',\n",
    "        'effective_cloud_cover:p', 'direct_rad_1h:J', 'snow_water:kgm2', 'is_in_shadow:idx',\n",
    "        'fresh_snow_24h:cm', 'wind_speed_u_10m:ms', 'total_cloud_cover:p', 'msl_pressure:hPa',\n",
    "        'is_day:idx', 'relative_humidity_1000hPa:p', 'pressure_100m:hPa', 'ceiling_height_agl:m'\n",
    "    ]\n",
    "\n",
    "    interactions = {}\n",
    "    ratios = {}\n",
    "    differences = {}\n",
    "    lags = {}\n",
    "    self_interactions = {}\n",
    "    additive = {}\n",
    "\n",
    "    for col_pair in itertools.combinations(relevant_features, 2):\n",
    "        interactions[f'{col_pair[0]}_times_{col_pair[1]}'] = data[col_pair[0]] * data[col_pair[1]]\n",
    "        ratios[f'{col_pair[0]}_div_{col_pair[1]}'] = data[col_pair[0]] / (data[col_pair[1]] + 1e-8)\n",
    "        differences[f'{col_pair[0]}_minus_{col_pair[1]}'] = data[col_pair[0]] - data[col_pair[1]]\n",
    "        additive[f'{col_pair[0]}_plus_{col_pair[1]}'] = data[col_pair[0]] + data[col_pair[1]]\n",
    "\n",
    "    for col in relevant_features:\n",
    "        self_interactions[f'{col}_squared'] = data[col] ** 2\n",
    "\n",
    "    # Creating lags for all relevant features\n",
    "    for col in relevant_features:\n",
    "        lags[f'{col}_lag1'] = data[col].shift(1)\n",
    "        lags[f'{col}_lag3'] = data[col].shift(3)\n",
    "\n",
    "    # Concatenate all new features with the original data\n",
    "    data = pd.concat([data, pd.DataFrame(interactions), pd.DataFrame(ratios),\n",
    "                      pd.DataFrame(differences), pd.DataFrame(lags), pd.DataFrame(self_interactions), pd.DataFrame(additive)], axis=1)\n",
    "\n",
    "    data['wind_magnitude'] = np.sqrt(data['wind_speed_u_10m:ms']**2 + data['wind_speed_v_10m:ms']**2)\n",
    "    data['wind_direction'] = np.arctan2(data['wind_speed_v_10m:ms'], data['wind_speed_u_10m:ms'])\n",
    "    data['solar_angle_impact'] = np.sin(np.radians(data['sun_elevation:d']))\n",
    "\n",
    "    return data\n",
    "\n",
    "def generate_solar_features_3(data):\n",
    "    # These features we got by running AutoGluon feature importance on the original dataset and choosing the top 20\n",
    "    relevant_features = [\n",
    "        'clear_sky_rad:W', 'clear_sky_energy_1h:J', 'direct_rad_1h:J', 'sun_elevation:d',\n",
    "        'direct_rad:W', 'diffuse_rad:W', 'sun_azimuth:d', 'diffuse_rad_1h:J',\n",
    "        'air_density_2m:kgm3', 'wind_speed_v_10m:ms', 'fresh_snow_24h:cm',\n",
    "        'relative_humidity_1000hPa:p', 'total_cloud_cover:p', 'effective_cloud_cover:p',\n",
    "        'cloud_base_agl:m', 'snow_water:kgm2', 't_1000hPa:K', 'is_in_shadow:idx', 'dew_point_2m:K',\n",
    "        'pressure_100m:hPa'\n",
    "    ]\n",
    "\n",
    "    interactions = {}\n",
    "    ratios = {}\n",
    "    differences = {}\n",
    "    lags = {}\n",
    "    self_interactions = {}\n",
    "    additive = {}\n",
    "\n",
    "    for col_pair in itertools.combinations(relevant_features, 2):\n",
    "        interactions[f'{col_pair[0]}_times_{col_pair[1]}'] = data[col_pair[0]] * data[col_pair[1]]\n",
    "        ratios[f'{col_pair[0]}_div_{col_pair[1]}'] = data[col_pair[0]] / (data[col_pair[1]] + 1e-8)\n",
    "        differences[f'{col_pair[0]}_minus_{col_pair[1]}'] = data[col_pair[0]] - data[col_pair[1]]\n",
    "        additive[f'{col_pair[0]}_plus_{col_pair[1]}'] = data[col_pair[0]] + data[col_pair[1]]\n",
    "\n",
    "    for col in relevant_features:\n",
    "        self_interactions[f'{col}_squared'] = data[col] ** 2\n",
    "\n",
    "    # Creating lags for all relevant features\n",
    "    for col in relevant_features:\n",
    "        lags[f'{col}_lag1'] = data[col].shift(1)\n",
    "        lags[f'{col}_lag3'] = data[col].shift(3)\n",
    "\n",
    "    # Concatenate all new features with the original data\n",
    "    data = pd.concat([data, pd.DataFrame(interactions), pd.DataFrame(ratios),\n",
    "                      pd.DataFrame(differences), pd.DataFrame(lags), pd.DataFrame(self_interactions), pd.DataFrame(additive)], axis=1)\n",
    "\n",
    "    data['wind_magnitude'] = np.sqrt(data['wind_speed_u_10m:ms']**2 + data['wind_speed_v_10m:ms']**2)\n",
    "    data['wind_direction'] = np.arctan2(data['wind_speed_v_10m:ms'], data['wind_speed_u_10m:ms'])\n",
    "    data['solar_angle_impact'] = np.sin(np.radians(data['sun_elevation:d']))\n",
    "\n",
    "    return data\n",
    "\n",
    "def closest_impute(series):\n",
    "\n",
    "    ffill = series.fillna(method='ffill')\n",
    "    bfill = series.fillna(method='bfill')\n",
    "\n",
    "    # Calculate the distances to the nearest non-NaN values\n",
    "    ffill_dist = series.index.to_series().fillna(method='ffill') - series.index.to_series()\n",
    "    bfill_dist = series.index.to_series().fillna(method='bfill') - series.index.to_series()\n",
    "\n",
    "    # Where the forward fill distance is smaller or equal, use ffill, otherwise use bfill\n",
    "    combined = np.where(ffill_dist <= bfill_dist, ffill, bfill)\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOCATION A"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15a87f4fa89686f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# First baseline model\n",
    "Here we just tried to create our first XGBoost model and some feature removal based on observations from EDA."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88f736aa9e65ddf5"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:02:20.585541300Z",
     "start_time": "2023-11-11T14:02:12.863940400Z"
    }
   },
   "id": "a527a25f5d1dd6d"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "data": {
      "text/plain": "193.9676197393052"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['pv_measurement', 'time', 'date_forecast', 'date_calc']), df['pv_measurement'], test_size=0.2, random_state=42)\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:02:20.586541500Z",
     "start_time": "2023-11-11T14:02:12.985160500Z"
    }
   },
   "id": "e006479617413a61"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:02:20.586541500Z",
     "start_time": "2023-11-11T14:02:14.271111800Z"
    }
   },
   "id": "333d97cb870863dd"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "data": {
      "text/plain": "196.24763684782764"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['pv_measurement', 'time', 'date_forecast', 'date_calc']), df['pv_measurement'], test_size=0.2, random_state=42)\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:02:20.586541500Z",
     "start_time": "2023-11-11T14:02:14.366878200Z"
    }
   },
   "id": "8204f1d94154d613"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:02:20.586541500Z",
     "start_time": "2023-11-11T14:02:15.541280300Z"
    }
   },
   "id": "b7998b20eb7cb6fa"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "data": {
      "text/plain": "196.24763684782764"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['pv_measurement', 'time', 'date_forecast', 'date_calc']), df['pv_measurement'], test_size=0.2, random_state=42)\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:02:20.586541500Z",
     "start_time": "2023-11-11T14:02:15.636346600Z"
    }
   },
   "id": "d7f98f0541d094e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data splitting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcc3589a2f86b25b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting where observed stops"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7a5df5082398fa7"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:02:20.586541500Z",
     "start_time": "2023-11-11T14:02:16.800858900Z"
    }
   },
   "id": "1c2a6ef5f3f2b8b8"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "data": {
      "text/plain": "136.62809603726367"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "validation_df = df[df['time'] > train_end_date]\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:02:20.587541200Z",
     "start_time": "2023-11-11T14:02:16.915143200Z"
    }
   },
   "id": "502a38bafc998431"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train on all of observed and random 50% of estimated"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de3e156273a18f96"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:02:20.587541200Z",
     "start_time": "2023-11-11T14:02:18.176852400Z"
    }
   },
   "id": "ff61ca7a4aed0934"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "C:\\Users\\marku\\anaconda3\\envs\\Testing\\lib\\site-packages\\xgboost\\data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "data": {
      "text/plain": "120.22561436425184"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:02:20.587541200Z",
     "start_time": "2023-11-11T14:02:18.287742300Z"
    }
   },
   "id": "44682e115016cc7c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mean resampling\n",
    "We noticed that our data got automatically resampled to 1hr when we merged, potentially loosing a lot of info. To counter this we manually resampled it to mean of the hour. This lead to worse score on validation, but better on Kaggle."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c4aec3b20f54d85"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_a = resample_to_hourly(X_test_estimated_a)\n",
    "\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T14:02:20.587541200Z",
     "start_time": "2023-11-11T14:02:19.611814700Z"
    }
   },
   "id": "27550a100b070b6b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T14:02:19.753874100Z"
    }
   },
   "id": "f686340f511ad106"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imputing missing values\n",
    "Here we tested out three methods of handling missing values. Method 3 performed best on validation, but far worse on Kaggle."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e645a0738ee2919"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling missing values: Iterative imputer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f9f7342898338e1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_a = resample_to_hourly(X_test_estimated_a)\n",
    "\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6de021e13fb7be3d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(max_iter=50, random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c46492a07d71b0b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33bf83fbaf289607"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling missing values: Simple imputer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38f25cda4c5cf87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_a = resample_to_hourly(X_test_estimated_a)\n",
    "\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2be46cd3c5c8605"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols_to_impute = ['ceiling_height_agl:m']\n",
    "\n",
    "imputer = SimpleImputer()\n",
    "X_test_estimated_a[cols_to_impute] = imputer.fit_transform(X_test_estimated_a[cols_to_impute])\n",
    "df= imputer.fit_transform(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5ef8182f0c966bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c8841f204586676"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling missing values: Drop every row with NA-values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62094f8973054e4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_a = resample_to_hourly(X_test_estimated_a)\n",
    "\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a007ce549f8ac00f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b71fb3fc54369c85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c07801346e905faf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling missing values: Removing pv_measurement, ceiling_height & Iterative imputer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfe6d6c1173337a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_a = resample_to_hourly(X_test_estimated_a)\n",
    "\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d56bf702d8e917b4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44bdc8e8fcce961a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64f3a8468f482a5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# More predictors\n",
    "Here we tried some different models using the best stuff we found from previous steps. We didn't bother doing the extreme feature engineering due to training time."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d99f820295b0860"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing new models: RandomForestRegressor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65365febcf512439"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_a = resample_to_hourly(X_test_estimated_a)\n",
    "\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82e0dbe6557062fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(max_iter=50, random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e5f8b89e9186c1b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = is_estimated(df)\n",
    "X_test_estimated_a = is_estimated(X_test_estimated_a, 'date_forecast')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "680bdb1ba8b1edbe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38a39ab8d48efc10"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing new models: Extreme Trees"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45fb872faba6baa7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_a = resample_to_hourly(X_test_estimated_a)\n",
    "\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7ac4bb014418a40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(max_iter=50, random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c61a85319caed27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = is_estimated(df)\n",
    "X_test_estimated_a = is_estimated(X_test_estimated_a, 'date_forecast')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9754fe89b10dda01"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = ExtraTreesRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a6f0ac45f2adc7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering\n",
    "First attempt at creating some new features."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b3ab6415a65c103"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_a = resample_to_hourly(X_test_estimated_a)\n",
    "\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddfb41abba7f8b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1861851c3b8d1227"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = is_estimated(df)\n",
    "X_test_estimated_a = is_estimated(X_test_estimated_a, 'date_forecast')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1c8a97b9a75885f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21eb20e29a7806cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering with guiding from ChatGPT\n",
    "Here we gave ChatGPT the name of each column + a description of our problem at hand, then told GPT to \"create some smart features from these columns\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d70d55115e87d0a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_a = resample_to_hourly(X_test_estimated_a)\n",
    "\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "594f452d8eae38a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3727555ba0411d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = is_estimated(df)\n",
    "df = generate_wind_features(df)\n",
    "\n",
    "X_test_estimated_a = is_estimated(X_test_estimated_a, 'date_forecast')\n",
    "X_test_estimated_a = generate_wind_features(X_test_estimated_a)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bab2fe2a8969858"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7de10871f505d2fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Interpretation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eda64922fb03e053"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature names and their importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "sorted_feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(sorted_feature_importance_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5316bcaa2e4f8d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering EXTREME\n",
    "Creating A BUNCH of interaction terms. This idea we got from the Kaggle State of AI report 2023 where a team coming in 2nd place shared their strategies. We need however do a simplified version mostly due to compute time."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccd663106f18087"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_a = resample_to_hourly(X_test_estimated_a)\n",
    "\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73e77929866ae45f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(max_iter=50, random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0dc7a49a0a516d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = is_estimated(df)\n",
    "df = generate_solar_features_1(df)\n",
    "\n",
    "X_test_estimated_a = is_estimated(X_test_estimated_a, 'date_forecast')\n",
    "X_test_estimated_a = generate_solar_features_1(X_test_estimated_a)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c2205eb03576da4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8e047bc31abf5b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The introduction of AutoGluon"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "682571eccd2a8669"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AutoGluon with basic feature engineering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb4f18fbcb9e0a9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_a = resample_to_hourly(X_test_estimated_a)\n",
    "\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d10a3bf144dd0940"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d72cce1ec118a2ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = is_estimated(df)\n",
    "df = generate_wind_features(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0eb3a027ebd166d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality')\n",
    "\n",
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cd880e32b72f97f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AutoGluon with no imputations\n",
    "The automl tool has built in support for handling missing values, this is a test of that"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ab55ee73f731ddc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_a = resample_to_hourly(X_test_estimated_a)\n",
    "\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "590f7a3d65cadc88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = is_estimated(df)\n",
    "df = generate_wind_features(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48ef3480df5c7567"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality')\n",
    "\n",
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fb112f89355632c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AutoGluon with EXTREME feature engineering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81e303b376561760"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_a = resample_to_hourly(X_test_estimated_a)\n",
    "\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99ca078e92abea16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31ee5a2ad875fa12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = is_estimated(df)\n",
    "df = generate_solar_features_1(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c1de793936a7233"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality')\n",
    "\n",
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6990e074579b151e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AutoGluon with Feature selection after extreme feature engineering\n",
    "Validation got worse, but Kaggle results got a MASSIVE decrease"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1de195e63a9583d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_importance = predictor.feature_importance(val_data)\n",
    "\n",
    "best_features = feature_importance[feature_importance['importance'] > 0.1].index.tolist()\n",
    "\n",
    "X_train = X_train[best_features]\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "X_val = X_val[best_features]\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "label = 'pv_measurement'\n",
    "\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality')\n",
    "\n",
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a138531b4243888"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AutoGluon with better quality preset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d83fbf327fe7baf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_a = resample_to_hourly(X_test_estimated_a)\n",
    "\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ca17b99fc799e2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a5211e8f6929758"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = is_estimated(df)\n",
    "df = generate_wind_features(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55bde9e1b04f6256"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6030bb78c5551d43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor_1 = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1890f95ee12b1eab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = predictor_1.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec720514422726"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_importance = predictor_1.feature_importance(val_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5664bb1ce98195d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_features = feature_importance[feature_importance['importance'] > 0.1].index.tolist()\n",
    "\n",
    "X_train = X_train[best_features]\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "X_val = X_val[best_features]\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "label = 'pv_measurement'\n",
    "\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='best_quality', num_gpus=1, num_stack_levels=0, use_bag_holdout=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7730ca3e377d23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Take-aways from Location A:\n",
    "Use AutoGluon all the way. We are however slightly afraid of overfitting. Considering our validation score is in the 80's\n",
    "Use best_quality preset as this leads to better Kaggle scores when using wind feature engineering.\n",
    "\n",
    "Extreme feature engineering with feature selection had a good effect on Kaggle, however takes ages to train the model.\n",
    "\n",
    "Just because the validation score is reduced, does not mean the Kaggle score is. (Makes all of this a real pain in the ass)\n",
    "\n",
    "Resample the datasets to hourly by taking the mean of that hour.\n",
    "Remove each feature related to snow since they don't have any values 99.999999999% of the time, but keep fresh_sow_12h and fresh_snow_24h as it has some values.\n",
    "Remove each feature where every row contains the same value. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0ead0247562b4f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Location B"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e8d7fbc80de6351"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# First baseline model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec085cd94091a1f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2ed0b96605cd5b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2807bd314b2c74ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71fe1c220d637db3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c4bd773a1f8aba2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f78ecd943bb4c38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cff28839ffcc73b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mean resampling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d4c0995410f405e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_b = resample_to_hourly(X_test_estimated_b)\n",
    "\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "156d34ba2eef4b26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2301e9f3ad004888"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "691eb7ccf7d99e13"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imputing missing values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f26e360bd9f5b0d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling missing values: Iterative imputer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65ca6f1ad1c52f4d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_b = resample_to_hourly(X_test_estimated_b)\n",
    "\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d25a16f323352b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b994009f8e34e4fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74cdbd823293c665"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling missing values: Simple imputer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77fb160453ffe03c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_b = resample_to_hourly(X_test_estimated_b)\n",
    "\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83c2ee1ec8bbe873"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "cols_to_impute = ['ceiling_height_agl:m']\n",
    "\n",
    "imputer = SimpleImputer()\n",
    "X_test_estimated_a[cols_to_impute] = imputer.fit_transform(X_test_estimated_a[cols_to_impute])\n",
    "df= imputer.fit_transform(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55fc8d0dd00502a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7e37e14a2538915"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling missing values: Drop every row with NA-values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "429c3284675823ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_b = resample_to_hourly(X_test_estimated_b)\n",
    "\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9d6995444bac0b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22826205a135eab1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbffb4804fd89729"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling missing values: Removing pv_measurement & Iterative imputer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9773d0b131f4e1b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_b = resample_to_hourly(X_test_estimated_b)\n",
    "\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf9304600dc3149f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4711b8c602e13810"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59922d118ae67dc1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Handling constant periods"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f041c9465269879"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_b = resample_to_hourly(X_test_estimated_b)\n",
    "\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad94a640dfc917d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b307fd7f09ddd09"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segments = find_long_constant_periods(train_b['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "df = remove_unwanted_rows(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3626b1600438de7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f94576b523a8c9d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa0099863a7373f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_b = resample_to_hourly(X_test_estimated_b)\n",
    "\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fae6453d17ea611"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "836a6704b8833980"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = is_estimated(df)\n",
    "segments = find_long_constant_periods(train_b['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "df = remove_unwanted_rows(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ad56d42f4aaa07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af1f248fbaa798c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature creation with guiding from ChatGPT\n",
    "Here we gave ChatGPT the name of each column + a description of our problem at hand, then told GPT to \"create some smart features from these columns\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2810a5bdcedd533f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_b = resample_to_hourly(X_test_estimated_b)\n",
    "\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91bedb3b76f4acbe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e72442731d71e63c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = is_estimated(df)\n",
    "df = generate_wind_features(df)\n",
    "segments = find_long_constant_periods(train_b['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "df = remove_unwanted_rows(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d0aeec2c055deef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "368758d793ba0fb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature importance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d3b6999965188fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature names and their importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "sorted_feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(sorted_feature_importance_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e292ad1ab5f7d08a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "From this feature importance we can see that different features are important for different locations. The reason for this is hard to conclude with."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d65111e1156eb5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering EXTREME\n",
    "Creating A BUNCH of interaction terms. This idea we got from the Kaggle State of AI report 2023 where a team coming in 2nd place shared their strategies. We need however do a simplified version mostly due to compute time."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d56e39953a37e0f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_b = resample_to_hourly(X_test_estimated_b)\n",
    "\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65a05d34ad4b5914"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a5ccba517de20e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = is_estimated(df)\n",
    "df = generate_solar_features_2(df)\n",
    "segments = find_long_constant_periods(train_b['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "df = remove_unwanted_rows(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa727abcacac560d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "960b9175427bd5ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The introduction of AutoGluon"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0180cad22e689de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_b = resample_to_hourly(X_test_estimated_b)\n",
    "\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12b16e90cb6e8879"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd2e8d117d775986"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segments = find_long_constant_periods(train_b['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "df = remove_unwanted_rows(df)\n",
    "df = is_estimated(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56b1371bc4812a6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "426969791d0f038"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AutoGluon with no imputations\n",
    "The automl tool has built in support for handling missing values, this is a test of that"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1b9ef5fab55d594"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_b = resample_to_hourly(X_test_estimated_b)\n",
    "\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5138d605a474a31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segments = find_long_constant_periods(train_b['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "df = remove_unwanted_rows(df)\n",
    "df = is_estimated(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cf36b5bf2a02277"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality')\n",
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31a9d01f0293df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AutoGluon with EXTREME feature engineering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ba8faffa8ae25ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_b = resample_to_hourly(X_test_estimated_b)\n",
    "\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9492a73ab2af1251"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(max_iter=50, random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51a9fc1d5eaf8407"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segments = find_long_constant_periods(train_b['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "df = remove_unwanted_rows(df)\n",
    "df = is_estimated(df)\n",
    "df = generate_solar_features_2(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e926617a01a00f3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality')\n",
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8281e3775857d27"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AutoGluon with Feature selection after extreme feature engineering\n",
    "Validation got worse, but Kaggle results got a decrease"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c9daf7e30a95f9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_importance = predictor.feature_importance(val_data)\n",
    "\n",
    "best_features = feature_importance[feature_importance['importance'] > 0.1].index.tolist()\n",
    "\n",
    "X_train = X_train[best_features]\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "X_val = X_val[best_features]\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "label = 'pv_measurement'\n",
    "\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality')\n",
    "\n",
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba9774b4076d2b19"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AutoGluon with better quality preset\n",
    "This lead to worse results on Kaggle over using medium preset. Probably due to overfitting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab323b6ecaa69a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_b = resample_to_hourly(X_test_estimated_b)\n",
    "\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf48acd84df94a2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(max_iter=50, random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24b6458d6eebbef9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segments = find_long_constant_periods(train_b['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "df = remove_unwanted_rows(df)\n",
    "df = is_estimated(df)\n",
    "df = generate_wind_features(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8b28a1056eac19b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='best_quality', num_gpus=1, num_stack_levels=0, use_bag_holdout=True)\n",
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7db8f42d19913056"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Take-aways from Location B\n",
    "Use AutoGluon all the way\n",
    "\n",
    "What works in Location A does not always work in B\n",
    "Don't use extreme feature engineering as this DESTROYS the Kaggle result.\n",
    "Use the best quality preset when training.\n",
    "\n",
    "Remove snow related features except 12h and 24h.\n",
    "Remove features which is always the same value.\n",
    "Remove periods with constant values.\n",
    "Remove \"unwanted rows\" where some key power production features are 0, but pv_measurement is over a threshold."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d87c1f50619db42"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Location C"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab237a96a6748180"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_c = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_c = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "X_train_observed_c = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_c, X_train_estimated_c])\n",
    "\n",
    "df = pd.merge(df, train_c, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98678cdf6e1038bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c11b8f5a82c1ea54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_c = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_c = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "X_train_observed_c = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_c, X_train_estimated_c])\n",
    "\n",
    "df = pd.merge(df, train_c, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'rain_water:kgm2', 'snow_drift:idx', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da105534a482e69e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83c30106a49f90af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Resampling to hourly"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "591fc2a241ad28f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_c = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_c = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "X_train_observed_c = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_c, X_train_estimated_c])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "\n",
    "df = pd.merge(df, train_c, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'rain_water:kgm2', 'snow_drift:idx', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8dbae2e4a8c7f7c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eed090ef9fd8135d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling missing values: Iterative imputer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cff112becb779d19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_c = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_c = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "X_train_observed_c = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_c, X_train_estimated_c])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "\n",
    "df = pd.merge(df, train_c, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'rain_water:kgm2', 'snow_drift:idx', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bef090f4183a4ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2bf63cd9e66f15b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd7be97a3fa34e3b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature engineering with guiding from ChatGPT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "920d87bd0295d4ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_c = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_c = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "X_train_observed_c = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_c, X_train_estimated_c])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "\n",
    "df = pd.merge(df, train_c, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'rain_water:kgm2', 'snow_drift:idx', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dff65430b1d78f0c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17147f1abee24e30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = is_estimated(df)\n",
    "df = generate_wind_features(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a24725822b005030"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c29d5f2759420659"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Handling constant periods"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "308c8908c7d99224"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_c = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_c = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "X_train_observed_c = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_c, X_train_estimated_c])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "\n",
    "df = pd.merge(df, train_c, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'rain_water:kgm2', 'snow_drift:idx', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d76e1bc5121ded1b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51d2d787e1c7c7f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segments = find_long_constant_periods(train_b['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "df = remove_unwanted_rows(df)\n",
    "df = is_estimated(df)\n",
    "df = generate_wind_features(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ca75782b224f52e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mae"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1de130f462dccdc6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature importance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "247d5d237019a5dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature names and their importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "sorted_feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(sorted_feature_importance_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6f5f86153fca54"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we can see similar patterns as in Location B, where other features are important than in Location A. However, B and C is very close. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f417f8c255a1028a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AutoGluon"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9fb0098918b2c30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_c = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_c = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "X_train_observed_c = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_c, X_train_estimated_c])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "\n",
    "df = pd.merge(df, train_c, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'rain_water:kgm2', 'snow_drift:idx', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "445c9d8e3a376c60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b66d315ad6ccdd79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segments = find_long_constant_periods(train_b['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "df = remove_unwanted_rows(df)\n",
    "df = is_estimated(df)\n",
    "df = generate_wind_features(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "131a8ae320ad2843"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=100)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51c8f7b3d09ea43c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6a127b373e9ec5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_c = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_c = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "X_train_observed_c = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_c, X_train_estimated_c])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "\n",
    "df = pd.merge(df, train_c, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_calc', 'snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'rain_water:kgm2', 'snow_drift:idx', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7da060db4c9420b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b179e16acbcfa8c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segments = find_long_constant_periods(train_b['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "df = remove_unwanted_rows(df)\n",
    "df = is_estimated(df)\n",
    "df = generate_wind_features(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d23fb4a07a34136b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=100)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "\n",
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='best_quality', num_gpus=1, num_stack_levels=0, use_bag_holdout=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6bd8bb35439b7e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3202823e842ece20"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Take-aways from Location C\n",
    "Use AutoGluon all the way.\n",
    "\n",
    "Location B and C are very similar and what works in one often work in the other."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21b0d242dbd095b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
