{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:27:21.800946100Z",
     "start_time": "2023-10-24T06:27:21.761950400Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "X_test_estimated_a['date_forecast'] = pd.to_datetime(X_test_estimated_a['date_forecast'])\n",
    "X_test_estimated_a = X_test_estimated_a[X_test_estimated_a['date_forecast'].dt.minute == 0]\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_forecast', 'date_calc', 'snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'fresh_snow_24h:cm', 'snow_depth:cm', 'snow_melt_10min:mm', 'snow_water:kgm2', 'wind_speed_w_1000hPa:ms'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:27:21.858183600Z",
     "start_time": "2023-10-24T06:27:21.778947100Z"
    }
   },
   "id": "7dddf00518babb04"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[['ceiling_height_agl:m', 'cloud_base_agl:m']] = imputer.fit_transform(df[['ceiling_height_agl:m', 'cloud_base_agl:m']])\n",
    "X_test_estimated_a[['ceiling_height_agl:m', 'cloud_base_agl:m']] = imputer.fit_transform(X_test_estimated_a[['ceiling_height_agl:m', 'cloud_base_agl:m']])\n",
    "X_test_estimated_a = X_test_estimated_a.rename(columns={'date_forecast': 'time'})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:27:21.903040Z",
     "start_time": "2023-10-24T06:27:21.858950300Z"
    }
   },
   "id": "5b09941030a46e5"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "threshold = 0.98\n",
    "\n",
    "segments = find_long_constant_periods(train_a['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "\n",
    "df = lag_features_by_one_hour(df, ['diffuse_rad_1h:J', 'direct_rad_1h:J', 'clear_sky_energy_1h:J'])\n",
    "df = is_estimated(df)\n",
    "\n",
    "X_test_estimated_a = lag_features_by_one_hour(X_test_estimated_a, ['diffuse_rad_1h:J', 'direct_rad_1h:J', 'clear_sky_energy_1h:J'])\n",
    "X_test_estimated_a = is_estimated(X_test_estimated_a)\n",
    "common_columns = df.columns.intersection(X_test_estimated_a.columns)\n",
    "X_test_estimated_a = X_test_estimated_a.loc[:, common_columns]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:27:22.031284700Z",
     "start_time": "2023-10-24T06:27:21.950908Z"
    }
   },
   "id": "575c134f04b12ffd"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# # Define the split date\n",
    "# split_date = '2022-10-27'\n",
    "# \n",
    "# # Convert the 'time' column to a datetime object\n",
    "# df['time'] = pd.to_datetime(df['time'])\n",
    "# \n",
    "# # Sorting the data by the 'time' column to maintain chronological order\n",
    "# df.sort_values('time', inplace=True)\n",
    "# \n",
    "# # Splitting the data into training and test sets based on the split date\n",
    "# train_df = df[df['time'] < split_date]\n",
    "# test_df = df[df['time'] >= split_date]\n",
    "# \n",
    "# # Identifying the features and the target variable\n",
    "# X_train = train_df.drop(columns=['pv_measurement', 'time'])\n",
    "# y_train = train_df['pv_measurement']\n",
    "# X_test = test_df.drop(columns=['pv_measurement', 'time'])\n",
    "# y_test = test_df['pv_measurement']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:27:22.048385200Z",
     "start_time": "2023-10-24T06:27:22.033252400Z"
    }
   },
   "id": "3a90e6dbe4b1c0ee"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "# 2023-01-29\n",
    "validation_end_date = '2023-01-29'\n",
    "# 2023-03-16\n",
    "# Convert 'time' column to datetime, if not already\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Split the data into training, validation, and testing sets based on the new split dates\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "validation_df = df[(df['time'] >= train_end_date) & (df['time'] < validation_end_date)]\n",
    "test_df = df[df['time'] >= train_end_date]\n",
    "\n",
    "# Randomly sample data within these periods (assuming you want to keep the same data structure)\n",
    "# train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# validation_df = validation_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "X_test = test_df.drop(columns=['pv_measurement', 'time'])\n",
    "y_test = test_df['pv_measurement']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:27:22.078473700Z",
     "start_time": "2023-10-24T06:27:22.048385200Z"
    }
   },
   "id": "735c00d28fb45964"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231024_062722\\\"\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231024_062722\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Disk Space Avail:   951.46 GB / 2047.46 GB (46.5%)\n",
      "Train Data Rows:    29666\n",
      "Train Data Columns: 36\n",
      "Tuning Data Rows:    2187\n",
      "Tuning Data Columns: 36\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 674.16825, 1195.54547)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    53864.71 MB\n",
      "\tTrain Data (Original)  Memory Usage: 4.71 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['elevation:m']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 34 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', [])   :  1 | ['is_estimated']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 32 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool']) :  3 | ['is_day:idx', 'is_in_shadow:idx', 'is_estimated']\n",
      "\t0.1s = Fit runtime\n",
      "\t35 features in original data used to generate 35 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 4.17 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Warning: use_bag_holdout=True, but bagged mode is not enabled. use_bag_holdout will be ignored.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-130.4637\t = Validation score   (-mean_absolute_error)\n",
      "\t4.58s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-129.6894\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t-57.4934\t = Validation score   (-mean_absolute_error)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t-56.0312\t = Validation score   (-mean_absolute_error)\n",
      "\t0.7s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-52.6134\t = Validation score   (-mean_absolute_error)\n",
      "\t11.26s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-65.8577\t = Validation score   (-mean_absolute_error)\n",
      "\t1.87s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-52.0904\t = Validation score   (-mean_absolute_error)\n",
      "\t2.27s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-65.1684\t = Validation score   (-mean_absolute_error)\n",
      "\t25.1s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-52.5922\t = Validation score   (-mean_absolute_error)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-47.3876\t = Validation score   (-mean_absolute_error)\n",
      "\t50.4s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t-53.6484\t = Validation score   (-mean_absolute_error)\n",
      "\t1.71s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-46.427\t = Validation score   (-mean_absolute_error)\n",
      "\t0.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 100.44s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231024_062722\\\")\n"
     ]
    }
   ],
   "source": [
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality', use_bag_holdout=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:29:02.592533800Z",
     "start_time": "2023-10-24T06:27:22.079540500Z"
    }
   },
   "id": "8cdc01fdb2478458"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                  model   score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2  -46.426989       0.082468  61.864709                0.000000           0.204802            2       True         12\n",
      "1        NeuralNetTorch  -47.387622       0.019557  50.397829                0.019557          50.397829            1       True         10\n",
      "2         ExtraTreesMSE  -52.090404       0.096002   2.265758                0.096002           2.265758            1       True          7\n",
      "3               XGBoost  -52.592244       0.004002   0.455328                0.004002           0.455328            1       True          9\n",
      "4       RandomForestMSE  -52.613441       0.062910  11.262078                0.062910          11.262078            1       True          5\n",
      "5         LightGBMLarge  -53.648352       0.004000   1.705070                0.004000           1.705070            1       True         11\n",
      "6              LightGBM  -56.031169       0.002001   0.704118                0.002001           0.704118            1       True          4\n",
      "7            LightGBMXT  -57.493395       0.003002   0.606132                0.003002           0.606132            1       True          3\n",
      "8       NeuralNetFastAI  -65.168424       0.023999  25.104208                0.023999          25.104208            1       True          8\n",
      "9              CatBoost  -65.857719       0.004999   1.867850                0.004999           1.867850            1       True          6\n",
      "10       KNeighborsDist -129.689414       0.058999   0.030998                0.058999           0.030998            1       True          2\n",
      "11       KNeighborsUnif -130.463682       0.166023   4.581660                0.166023           4.581660            1       True          1\n",
      "Number of models trained: 12\n",
      "Types of models trained:\n",
      "{'XTModel', 'CatBoostModel', 'RFModel', 'TabularNeuralNetTorchModel', 'KNNModel', 'XGBoostModel', 'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])     : 32 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "('int', ['bool']) :  3 | ['is_day:idx', 'is_in_shadow:idx', 'is_estimated']\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\Desktop\\Envs\\MLenv\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:169: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    }
   ],
   "source": [
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:29:03.412530600Z",
     "start_time": "2023-10-24T06:29:02.572501600Z"
    }
   },
   "id": "a7e2c6b4f360233f"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(X_test_estimated_a)\n",
    "y_pred = y_pred.clip(lower=0)\n",
    "y_pred = y_pred.reset_index(drop=True)\n",
    "y_pred.index.name = 'id'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:29:03.709091400Z",
     "start_time": "2023-10-24T06:29:03.388483700Z"
    }
   },
   "id": "bfe3dfacd845e4c5"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df.to_csv('result_a.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:29:03.723902500Z",
     "start_time": "2023-10-24T06:29:03.709091400Z"
    }
   },
   "id": "648334688fa703c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
