{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Team description\n",
    "\n",
    "Team: Ehhhhhhh\n",
    "\n",
    "Markus Kinn, 106660\n",
    "Mario Haroun, 543915\n",
    "Torstein Korten, 543955"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64904fa979ec069e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import itertools"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd54e68dff6bd457"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_unwanted_rows(df):\n",
    "    unwanted_rows = (df['direct_rad:W'] == 0) & (df['diffuse_rad:W'] == 0) & (df['pv_measurement'] > 200) & (df['sun_elevation:d'] < 0) & (df['is_day:idx'] == 0)\n",
    "    cleaned_df = df[~unwanted_rows]\n",
    "    return cleaned_df\n",
    "\n",
    "def find_long_constant_periods(data, threshold):\n",
    "    start = None\n",
    "    segments = []\n",
    "    for i in range(1, len(data)):\n",
    "        if data[i] == data[i-1] and data[i] != 0:\n",
    "            if start is None:\n",
    "                start = i-1\n",
    "        else:\n",
    "            if start is not None:\n",
    "                if (i - start) > threshold:\n",
    "                    segments.append((start, i))\n",
    "                start = None\n",
    "    return segments\n",
    "\n",
    "def remove_constant_periods(df, segments):\n",
    "    drop_indices = []\n",
    "    for start, end in segments:\n",
    "        drop_indices.extend(range(start, end))\n",
    "    return df.drop(drop_indices)\n",
    "\n",
    "def lag_features_by_one_hour(df, column_names, time_col='time'):\n",
    "\n",
    "    # Check if the DataFrame has a time-based index\n",
    "    df['index'] = df[time_col]\n",
    "    df = df.set_index('index')\n",
    "\n",
    "    # Loop through each column name to create a lagged feature\n",
    "    for col in column_names:\n",
    "        lagged_col_name = f\"{col}\"\n",
    "        df[lagged_col_name] = df[col].shift(freq='-1H')\n",
    "\n",
    "    return df\n",
    "\n",
    "def is_estimated(df, time_col='time'):\n",
    "    split_date = '2022-10-27'\n",
    "    df['is_estimated'] = 0  # Initialize with 0 (indicating observed)\n",
    "    df.loc[df[time_col] >= pd.Timestamp(split_date), 'is_estimated'] = 1  # Set 1 for estimated data\n",
    "    return df\n",
    "\n",
    "def resample_to_hourly(df, datetime_column='date_forecast'):\n",
    "    df[datetime_column] = pd.to_datetime(df[datetime_column])\n",
    "    df.sort_values(by=datetime_column, inplace=True)\n",
    "\n",
    "    df.set_index(datetime_column, inplace=True)\n",
    "\n",
    "    df_hourly = df.resample('H').mean()\n",
    "\n",
    "    df_hourly.dropna(how='all', inplace=True)\n",
    "\n",
    "    df_hourly.reset_index(inplace=True)\n",
    "\n",
    "    return df_hourly\n",
    "\n",
    "def generate_simple_features(data):\n",
    "    data['wind_magnitude'] = np.sqrt(data['wind_speed_u_10m:ms']**2 + data['wind_speed_v_10m:ms']**2)\n",
    "    data['wind_direction'] = np.arctan2(data['wind_speed_v_10m:ms'], data['wind_speed_u_10m:ms'])\n",
    "    data['solar_angle_impact'] = np.sin(np.radians(data['sun_elevation:d']))\n",
    "\n",
    "    data = data.drop(columns=['wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_10m:ms',\n",
    "                              'wind_speed_w_1000hPa:ms', 'wind_speed_v_10m:ms'])\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8bfe9b1fa24fee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Location A"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d138b9148270a977"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\Desktop\\NTNU\\ML\\Testing\\utils.py:58: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n",
      "C:\\Users\\marku\\Desktop\\NTNU\\ML\\Testing\\utils.py:58: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n"
     ]
    }
   ],
   "source": [
    "train_a = pd.read_parquet('./data/A/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('./data/A/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('./data/A/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('./data/A/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_a, X_train_estimated_a])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_a = resample_to_hourly(X_test_estimated_a)\n",
    "\n",
    "df = pd.merge(df, train_a, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])\n",
    "X_test_estimated_a = X_test_estimated_a.drop(columns=['snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm', 'elevation:m', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5af8154bc34a0c1"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d50db6a63ad66b44"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df = is_estimated(df)\n",
    "df = generate_simple_features(df)\n",
    "\n",
    "X_test_estimated_a = is_estimated(X_test_estimated_a, 'date_forecast')\n",
    "X_test_estimated_a = generate_simple_features(X_test_estimated_a)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "893714488c197742"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=50)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ead6312a920e8198"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231112_122520\\\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231112_122520\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Disk Space Avail:   906.18 GB / 2047.46 GB (44.3%)\n",
      "Train Data Rows:    31863\n",
      "Train Data Columns: 37\n",
      "Tuning Data Rows:    2197\n",
      "Tuning Data Columns: 37\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 650.65228, 1178.80649)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    49955.84 MB\n",
      "\tTrain Data (Original)  Memory Usage: 10.08 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 36 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', [])   :  1 | ['is_estimated']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 36 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['is_estimated']\n",
      "\t0.2s = Fit runtime\n",
      "\t37 features in original data used to generate 37 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 9.84 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.2s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\t-158.3291\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.69s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\t-157.5592\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-89.3237\t = Validation score   (-mean_absolute_error)\n",
      "\t411.2s\t = Training   runtime\n",
      "\t9.58s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-92.0423\t = Validation score   (-mean_absolute_error)\n",
      "\t367.74s\t = Training   runtime\n",
      "\t10.39s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ...\n",
      "\t-103.2474\t = Validation score   (-mean_absolute_error)\n",
      "\t15.76s\t = Training   runtime\n",
      "\t0.96s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-106.5493\t = Validation score   (-mean_absolute_error)\n",
      "\t339.47s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
      "\t-105.6914\t = Validation score   (-mean_absolute_error)\n",
      "\t2.61s\t = Training   runtime\n",
      "\t0.98s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-104.8978\t = Validation score   (-mean_absolute_error)\n",
      "\t118.21s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-96.2988\t = Validation score   (-mean_absolute_error)\n",
      "\t82.34s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-88.9644\t = Validation score   (-mean_absolute_error)\n",
      "\t575.83s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-89.1478\t = Validation score   (-mean_absolute_error)\n",
      "\t1724.73s\t = Training   runtime\n",
      "\t13.55s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-85.0988\t = Validation score   (-mean_absolute_error)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3682.95s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231112_122520\\\")\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "label = 'pv_measurement'\n",
    "\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='best_quality', num_gpus=1, num_stack_levels=0, use_bag_holdout=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ee4ddbf3d428a44"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                     model   score_val  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L2  -85.098833      23.319916  2711.967278                0.000000           0.210576            2       True         12\n",
      "1    NeuralNetTorch_BAG_L1  -88.964440       0.187825   575.829899                0.187825         575.829899            1       True         10\n",
      "2     LightGBMLarge_BAG_L1  -89.147817      13.550098  1724.725685               13.550098        1724.725685            1       True         11\n",
      "3        LightGBMXT_BAG_L1  -89.323748       9.581992   411.201118                9.581992         411.201118            1       True          3\n",
      "4          LightGBM_BAG_L1  -92.042318      10.390874   367.736543               10.390874         367.736543            1       True          4\n",
      "5           XGBoost_BAG_L1  -96.298775       0.467416    82.341583                0.467416          82.341583            1       True          9\n",
      "6   RandomForestMSE_BAG_L1 -103.247359       0.963061    15.758406                0.963061          15.758406            1       True          5\n",
      "7   NeuralNetFastAI_BAG_L1 -104.897758       0.349068   118.206358                0.349068         118.206358            1       True          8\n",
      "8     ExtraTreesMSE_BAG_L1 -105.691388       0.984460     2.610969                0.984460           2.610969            1       True          7\n",
      "9          CatBoost_BAG_L1 -106.549300       0.058463   339.468573                0.058463         339.468573            1       True          6\n",
      "10   KNeighborsDist_BAG_L1 -157.559228       0.502144     0.044999                0.502144           0.044999            1       True          2\n",
      "11   KNeighborsUnif_BAG_L1 -158.329094       0.692564     0.030000                0.692564           0.030000            1       True          1\n",
      "Number of models trained: 12\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_LGB', 'StackerEnsembleModel_NNFastAiTabular', 'StackerEnsembleModel_KNN', 'StackerEnsembleModel_XGBoost', 'StackerEnsembleModel_TabularNeuralNetTorch', 'WeightedEnsembleModel', 'StackerEnsembleModel_XT', 'StackerEnsembleModel_RF', 'StackerEnsembleModel_CatBoost'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])     : 36 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "('int', ['bool']) :  1 | ['is_estimated']\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\Desktop\\Envs\\MLenv\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:169: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    }
   ],
   "source": [
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e05e39cda3500227"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(X_test_estimated_a)\n",
    "y_pred = y_pred.clip(lower=0)\n",
    "y_pred = y_pred.reset_index(drop=True)\n",
    "y_pred.index.name = 'id'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89c2acf4ae20103b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df.to_csv('result_a.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df6705a7424a6083"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Location B"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1d9afa88fa437ff"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\Desktop\\NTNU\\ML\\Testing\\utils.py:58: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n",
      "C:\\Users\\marku\\Desktop\\NTNU\\ML\\Testing\\utils.py:58: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n"
     ]
    }
   ],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_b = resample_to_hourly(X_test_estimated_b)\n",
    "\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'snow_melt_10min:mm', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'fresh_snow_12h:cm', 'precip_5min:mm', 'rain_water:kgm2', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms'])\n",
    "X_test_estimated_b = X_test_estimated_b.drop(columns=['snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'snow_melt_10min:mm', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'fresh_snow_12h:cm', 'precip_5min:mm', 'rain_water:kgm2', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b32b8899af35732"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=123)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecae1e12da049f0b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "segments = find_long_constant_periods(train_b['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "df = remove_unwanted_rows(df)\n",
    "df = is_estimated(df)\n",
    "df = lag_features_by_one_hour(df, ['diffuse_rad_1h:J', 'direct_rad_1h:J', 'clear_sky_energy_1h:J'])\n",
    "\n",
    "X_test_estimated_b = is_estimated(X_test_estimated_b, 'date_forecast')\n",
    "X_test_estimated_b = lag_features_by_one_hour(X_test_estimated_b, ['diffuse_rad_1h:J', 'direct_rad_1h:J', 'clear_sky_energy_1h:J'], 'date_forecast')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59e58c94983f5056"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30749e6a0330a9e9"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231112_095423\\\"\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231112_095423\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Disk Space Avail:   897.90 GB / 2047.46 GB (43.9%)\n",
      "Train Data Rows:    27807\n",
      "Train Data Columns: 35\n",
      "Tuning Data Rows:    1801\n",
      "Tuning Data Columns: 35\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (1152.3, -0.0, 96.61183, 205.14064)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    55421.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 8.29 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 34 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', [])   :  1 | ['is_estimated']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 34 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['is_estimated']\n",
      "\t0.2s = Fit runtime\n",
      "\t35 features in original data used to generate 35 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 8.08 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.21s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-19.9231\t = Validation score   (-mean_absolute_error)\n",
      "\t4.49s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-19.9188\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 12.8494\n",
      "[2000]\tvalid_set's l1: 12.6426\n",
      "[3000]\tvalid_set's l1: 12.5573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-12.5173\t = Validation score   (-mean_absolute_error)\n",
      "\t5.3s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 12.9818\n",
      "[2000]\tvalid_set's l1: 12.8635\n",
      "[3000]\tvalid_set's l1: 12.8417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-12.828\t = Validation score   (-mean_absolute_error)\n",
      "\t4.58s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-14.2066\t = Validation score   (-mean_absolute_error)\n",
      "\t14.9s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-13.2545\t = Validation score   (-mean_absolute_error)\n",
      "\t9.63s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-13.6995\t = Validation score   (-mean_absolute_error)\n",
      "\t1.81s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-12.9604\t = Validation score   (-mean_absolute_error)\n",
      "\t19.81s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-13.658\t = Validation score   (-mean_absolute_error)\n",
      "\t0.8s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-10.8596\t = Validation score   (-mean_absolute_error)\n",
      "\t94.66s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 12.2347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-12.2166\t = Validation score   (-mean_absolute_error)\n",
      "\t6.65s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-10.5244\t = Validation score   (-mean_absolute_error)\n",
      "\t0.17s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 164.38s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231112_095423\\\")\n"
     ]
    }
   ],
   "source": [
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da28904d7154b6a"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                  model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2 -10.524436       0.169514  132.983032                0.000000           0.174001            2       True         12\n",
      "1        NeuralNetTorch -10.859605       0.018000   94.657072                0.018000          94.657072            1       True         10\n",
      "2         LightGBMLarge -12.216640       0.017002    6.645430                0.017002           6.645430            1       True         11\n",
      "3            LightGBMXT -12.517338       0.025510    5.300957                0.025510           5.300957            1       True          3\n",
      "4              LightGBM -12.828019       0.023003    4.582390                0.023003           4.582390            1       True          4\n",
      "5       NeuralNetFastAI -12.960386       0.023999   19.808258                0.023999          19.808258            1       True          8\n",
      "6              CatBoost -13.254452       0.004002    9.626988                0.004002           9.626988            1       True          6\n",
      "7               XGBoost -13.658010       0.004003    0.798718                0.004003           0.798718            1       True          9\n",
      "8         ExtraTreesMSE -13.699520       0.062000    1.814926                0.062000           1.814926            1       True          7\n",
      "9       RandomForestMSE -14.206601       0.063662   14.901998                0.063662          14.901998            1       True          5\n",
      "10       KNeighborsDist -19.918756       0.059998    0.042999                0.059998           0.042999            1       True          2\n",
      "11       KNeighborsUnif -19.923055       0.165998    4.486089                0.165998           4.486089            1       True          1\n",
      "Number of models trained: 12\n",
      "Types of models trained:\n",
      "{'KNNModel', 'XTModel', 'TabularNeuralNetTorchModel', 'WeightedEnsembleModel', 'XGBoostModel', 'NNFastAiTabularModel', 'RFModel', 'CatBoostModel', 'LGBModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])     : 34 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "('int', ['bool']) :  1 | ['is_estimated']\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\Desktop\\Envs\\MLenv\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:169: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    }
   ],
   "source": [
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28e806b4629925f9"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(X_test_estimated_b)\n",
    "y_pred = y_pred.clip(lower=0)\n",
    "y_pred = y_pred.reset_index(drop=True)\n",
    "y_pred.index.name = 'id'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "833988ad0ba5d69d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df.to_csv('result_b.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "124ea6705b31babe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Location C"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ca751e9031d6eb1"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\Desktop\\NTNU\\ML\\Testing\\utils.py:98: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n",
      "C:\\Users\\marku\\Desktop\\NTNU\\ML\\Testing\\utils.py:98: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_hourly = df.resample('H').mean()\n"
     ]
    }
   ],
   "source": [
    "train_c = pd.read_parquet('./data/C/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_c = pd.read_parquet('./data/C/X_train_estimated.parquet')\n",
    "X_train_observed_c = pd.read_parquet('./data/C/X_train_observed.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('./data/C/X_test_estimated.parquet')\n",
    "\n",
    "df = pd.concat([X_train_observed_c, X_train_estimated_c])\n",
    "\n",
    "df = resample_to_hourly(df)\n",
    "X_test_estimated_c = resample_to_hourly(X_test_estimated_c)\n",
    "\n",
    "df = pd.merge(df, train_c, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'rain_water:kgm2', 'snow_drift:idx', 'cloud_base_agl:m'])\n",
    "\n",
    "X_test_estimated_c = X_test_estimated_c.drop(columns=['snow_density:kgm3', 'elevation:m', 'snow_drift:idx', 'rain_water:kgm2', 'snow_drift:idx', 'cloud_base_agl:m'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e319f0dab115a372"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['pv_measurement'])\n",
    "\n",
    "datetime_features = df[['time', 'date_forecast']]\n",
    "df = df.drop(['time', 'date_forecast'], axis=1)\n",
    "\n",
    "imputer = IterativeImputer(random_state=123)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "df = pd.concat([df, datetime_features.reset_index(drop=True)], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6b815201256271c"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "segments = find_long_constant_periods(train_c['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "df = is_estimated(df)\n",
    "df = generate_simple_features(df)\n",
    "\n",
    "X_test_estimated_c = is_estimated(X_test_estimated_c, 'date_forecast')\n",
    "X_test_estimated_c = generate_simple_features(X_test_estimated_c)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec902df42900f59f"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "train_df = df[df['time'] < train_end_date]\n",
    "remaining_data = df[df['time'] > train_end_date]\n",
    "\n",
    "train_data, validation_df = train_test_split(remaining_data, test_size=0.5, random_state=100)\n",
    "train_df = pd.concat([train_df, train_data], ignore_index=True)\n",
    "\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time', 'date_forecast'])\n",
    "y_val = validation_df['pv_measurement']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a6f4e06252625ec"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231111_094818\\\"\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231111_094818\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Disk Space Avail:   904.58 GB / 2047.46 GB (44.2%)\n",
      "Train Data Rows:    24600\n",
      "Train Data Columns: 40\n",
      "Tuning Data Rows:    1465\n",
      "Tuning Data Columns: 40\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
      "\tLabel info (max, min, mean, stddev): (999.6, 0.0, 80.09705, 168.78947)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    53345.52 MB\n",
      "\tTrain Data (Original)  Memory Usage: 8.34 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 39 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', [])   :  1 | ['is_estimated']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 39 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['is_estimated']\n",
      "\t0.1s = Fit runtime\n",
      "\t40 features in original data used to generate 40 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 8.16 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-19.524\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-19.5595\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 11.2419\n",
      "[2000]\tvalid_set's l1: 10.495\n",
      "[3000]\tvalid_set's l1: 10.4243\n",
      "[4000]\tvalid_set's l1: 10.2802\n",
      "[5000]\tvalid_set's l1: 10.2093\n",
      "[6000]\tvalid_set's l1: 10.1382\n",
      "[7000]\tvalid_set's l1: 10.08\n",
      "[8000]\tvalid_set's l1: 10.0475\n",
      "[9000]\tvalid_set's l1: 10.0345\n",
      "[10000]\tvalid_set's l1: 10.017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-10.011\t = Validation score   (-mean_absolute_error)\n",
      "\t11.54s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 11.8042\n",
      "[2000]\tvalid_set's l1: 11.281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-11.2531\t = Validation score   (-mean_absolute_error)\n",
      "\t3.49s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-14.3738\t = Validation score   (-mean_absolute_error)\n",
      "\t9.02s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-11.4457\t = Validation score   (-mean_absolute_error)\n",
      "\t175.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-13.9507\t = Validation score   (-mean_absolute_error)\n",
      "\t1.52s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-12.985\t = Validation score   (-mean_absolute_error)\n",
      "\t35.95s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-12.6692\t = Validation score   (-mean_absolute_error)\n",
      "\t85.95s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-10.9963\t = Validation score   (-mean_absolute_error)\n",
      "\t194.29s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 11.7745\n",
      "[2000]\tvalid_set's l1: 11.5543\n",
      "[3000]\tvalid_set's l1: 11.5441\n",
      "[4000]\tvalid_set's l1: 11.5372\n",
      "[5000]\tvalid_set's l1: 11.5354\n",
      "[6000]\tvalid_set's l1: 11.5346\n",
      "[7000]\tvalid_set's l1: 11.5346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-11.5344\t = Validation score   (-mean_absolute_error)\n",
      "\t28.26s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-9.3701\t = Validation score   (-mean_absolute_error)\n",
      "\t0.17s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 549.54s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231111_094818\\\")\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "label = 'pv_measurement'\n",
    "\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76629db0960a34d9"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                  model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2  -9.370141       0.115195  385.202267                0.000000           0.172373            2       True         12\n",
      "1            LightGBMXT -10.011036       0.070190   11.542578                0.070190          11.542578            1       True          3\n",
      "2        NeuralNetTorch -10.996333       0.018003  194.288291                0.018003         194.288291            1       True         10\n",
      "3              LightGBM -11.253090       0.016001    3.491979                0.016001           3.491979            1       True          4\n",
      "4              CatBoost -11.445651       0.011000  175.707046                0.011000         175.707046            1       True          6\n",
      "5         LightGBMLarge -11.534445       0.067533   28.258561                0.067533          28.258561            1       True         11\n",
      "6               XGBoost -12.669188       0.186039   85.947021                0.186039          85.947021            1       True          9\n",
      "7       NeuralNetFastAI -12.985028       0.025512   35.949237                0.025512          35.949237            1       True          8\n",
      "8         ExtraTreesMSE -13.950658       0.077519    1.520458                0.077519           1.520458            1       True          7\n",
      "9       RandomForestMSE -14.373846       0.046999    9.016725                0.046999           9.016725            1       True          5\n",
      "10       KNeighborsUnif -19.524031       0.048511    0.029002                0.048511           0.029002            1       True          1\n",
      "11       KNeighborsDist -19.559494       0.030005    0.034998                0.030005           0.034998            1       True          2\n",
      "Number of models trained: 12\n",
      "Types of models trained:\n",
      "{'KNNModel', 'RFModel', 'NNFastAiTabularModel', 'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'XGBoostModel', 'CatBoostModel', 'XTModel', 'LGBModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])     : 39 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', ...]\n",
      "('int', ['bool']) :  1 | ['is_estimated']\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\Desktop\\Envs\\MLenv\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:169: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    }
   ],
   "source": [
    "results = predictor.fit_summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27cfee9a4ba2b862"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(X_test_estimated_c)\n",
    "y_pred = y_pred.clip(lower=0)\n",
    "y_pred = y_pred.reset_index(drop=True)\n",
    "y_pred.index.name = 'id'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0dc2ff2789084c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df.to_csv('result_c.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1923ca71ae7d5fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_a = pd.read_csv('result_a.csv')\n",
    "df_b = pd.read_csv('result_b.csv')\n",
    "df_c = pd.read_csv('result_c.csv')\n",
    "\n",
    "def combine_dataframes(df1, df2, df3):\n",
    "    # Concatenate the dataframes in the specified order\n",
    "    combined = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "    # Rename the 'index' column to 'id'\n",
    "    combined.rename(columns={'index': 'id'}, inplace=True)\n",
    "\n",
    "    # Ensure the 'id' values range from 0 to 'x'\n",
    "    combined['id'] = range(len(combined))\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "df = combine_dataframes(df_a, df_b, df_c)\n",
    "df.to_csv('result.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60e62c7faddbc7e9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
