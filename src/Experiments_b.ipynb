{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:29:27.567838200Z",
     "start_time": "2023-10-23T20:29:27.550021100Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac00ca6f300a78e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:29:27.622990800Z",
     "start_time": "2023-10-23T20:29:27.564837Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_b = pd.read_parquet('./data/B/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_b = pd.read_parquet('./data/B/X_train_estimated.parquet')\n",
    "X_train_observed_b = pd.read_parquet('./data/B/X_train_observed.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('./data/B/X_test_estimated.parquet')\n",
    "\n",
    "X_test_estimated_b['date_forecast'] = pd.to_datetime(X_test_estimated_b['date_forecast'])\n",
    "X_test_estimated_b = X_test_estimated_b[X_test_estimated_b['date_forecast'].dt.minute == 0]\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_test_estimated_b[['ceiling_height_agl:m', 'cloud_base_agl:m']] = imputer.fit_transform(X_test_estimated_b[['ceiling_height_agl:m', 'cloud_base_agl:m']])\n",
    "\n",
    "df = pd.concat([X_train_observed_b, X_train_estimated_b])\n",
    "df = pd.merge(df, train_b, left_on='date_forecast', right_on='time', how='inner')\n",
    "df = df.drop(columns=['date_forecast', 'date_calc', 'snow_density:kgm3', 'snow_drift:idx', 'fresh_snow_1h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'fresh_snow_24h:cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fcdf3064ae41f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:29:27.668988600Z",
     "start_time": "2023-10-23T20:29:27.624988900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_test_estimated_b[['ceiling_height_agl:m', 'cloud_base_agl:m']] = imputer.fit_transform(X_test_estimated_b[['ceiling_height_agl:m', 'cloud_base_agl:m']])\n",
    "df[['ceiling_height_agl:m', 'cloud_base_agl:m', 'pv_measurement']] = imputer.fit_transform(df[['ceiling_height_agl:m', 'cloud_base_agl:m', 'pv_measurement']])\n",
    "X_test_estimated_b = X_test_estimated_b.rename(columns={'date_forecast': 'time'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247b82798133aca5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:29:27.858021500Z",
     "start_time": "2023-10-23T20:29:27.714505400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold = 0.99\n",
    "\n",
    "segments = find_long_constant_periods(train_b['pv_measurement'], threshold=5)\n",
    "df = remove_constant_periods(df, segments)\n",
    "\n",
    "df = lag_features_by_one_hour(df, ['diffuse_rad_1h:J', 'direct_rad_1h:J', 'clear_sky_energy_1h:J'])\n",
    "df = is_estimated(df)\n",
    "# df = remove_highly_correlated_features(df, threshold)\n",
    "\n",
    "X_test_estimated_b = lag_features_by_one_hour(X_test_estimated_b, ['diffuse_rad_1h:J', 'direct_rad_1h:J', 'clear_sky_energy_1h:J'])\n",
    "X_test_estimated_b = is_estimated(X_test_estimated_b)\n",
    "common_columns = df.columns.intersection(X_test_estimated_b.columns)\n",
    "X_test_estimated_b = X_test_estimated_b.loc[:, common_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6735082845a46243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:29:27.874021100Z",
     "start_time": "2023-10-23T20:29:27.856021800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Define the split date\n",
    "# split_date = '2022-10-27'\n",
    "# \n",
    "# # Convert the 'time' column to a datetime object\n",
    "# df['time'] = pd.to_datetime(df['time'])\n",
    "# \n",
    "# # Sorting the data by the 'time' column to maintain chronological order\n",
    "# df.sort_values('time', inplace=True)\n",
    "# \n",
    "# # Splitting the data into training and test sets based on the split date\n",
    "# train_df = df[df['time'] < split_date]\n",
    "# test_df = df[df['time'] >= split_date]\n",
    "# \n",
    "# # Identifying the features and the target variable\n",
    "# X_train = train_df.drop(columns=['pv_measurement', 'time'])\n",
    "# y_train = train_df['pv_measurement']\n",
    "# X_test = test_df.drop(columns=['pv_measurement', 'time'])\n",
    "# y_test = test_df['pv_measurement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70fc47d464d38c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:29:27.908025500Z",
     "start_time": "2023-10-23T20:29:27.873022700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_end_date = '2022-10-21'\n",
    "# 2023-01-29\n",
    "validation_end_date = '2023-01-29'\n",
    "# 2023-03-16\n",
    "\n",
    "# Split the data into training, validation, and testing sets based on the new split dates\n",
    "train_df, val_and_test_df = split_df_on_date(df, train_end_date)\n",
    "validation_df, test_df = split_df_on_date(val_and_test_df, validation_end_date)\n",
    "\n",
    "# Randomly sample data within these periods (assuming you want to keep the same data structure)\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "validation_df = validation_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Use all time values for observed data\n",
    "train_df = mean_of_the_hour(train_df)\n",
    "validation_df = mean_of_the_hour(validation_df)\n",
    "test_df = mean_of_the_hour(test_df)\n",
    "\n",
    "# Identifying the features and the target variable\n",
    "X_train = train_df.drop(columns=['pv_measurement', 'time'])\n",
    "y_train = train_df['pv_measurement']\n",
    "X_val = validation_df.drop(columns=['pv_measurement', 'time'])\n",
    "y_val = validation_df['pv_measurement']\n",
    "X_test = test_df.drop(columns=['pv_measurement', 'time'])\n",
    "y_test = test_df['pv_measurement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6d9dd30e06e4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:53:08.051144600Z",
     "start_time": "2023-10-23T20:29:27.918314Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine training and validation data into a single dataset for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Specify the name of the target variable\n",
    "label = 'pv_measurement'\n",
    "\n",
    "# Create a TabularPredictor object\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"mean_absolute_error\").fit(train_data=train_data, tuning_data=val_data, presets='medium_quality', use_bag_holdout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb79b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predictions = predictor.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9bf8230058988",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:53:08.189661700Z",
     "start_time": "2023-10-23T20:53:08.053143700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = predictor.fit_summary(show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a66b06d0d305f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:53:08.455351800Z",
     "start_time": "2023-10-23T20:53:08.190663Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_estimated_b = mean_of_the_hour(X_test_estimated_b)\n",
    "\n",
    "y_pred = predictor.predict(X_test_estimated_b)\n",
    "y_pred = y_pred.clip(lower=0)\n",
    "y_pred = y_pred.reset_index(drop=True)\n",
    "y_pred.index.name = 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83feb22b3b78fa75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:53:08.473348900Z",
     "start_time": "2023-10-23T20:53:08.456350600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred)\n",
    "df.to_csv('result_b.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
